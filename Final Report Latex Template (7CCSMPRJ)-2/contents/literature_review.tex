\section{Literature Review} 

\subsection{SAT Solver Fundamentals}
The Davis-Putnam-Logemann-Loveland (DPLL) algorithm represents a fundamental breakthrough in solving Boolean satisfiability problems, building upon earlier approaches through systematic improvements. The original Davis-Putnam procedure, developed by Davis and Putnam in 1960, created a method for determining whether logical formulas could be satisfied using three key techniques: unit propagation, pure literal elimination, and resolution~\cite{davis1960computing}. This initial approach, whilst mathematically sound, suffered from exponential space complexity due to its reliance on resolution operations that could generate exponentially many intermediate clauses. The crucial evolutionary step occurred in 1962 when Davis, Logemann, and Loveland refined the approach by replacing the problematic resolution step with a systematic splitting strategy~\cite{davis1962machine}. This modification, which became known as the DPLL algorithm, maintained the efficient unit propagation and pure literal elimination rules whilst introducing chronological backtracking search. The algorithm operates by selecting unassigned variables, recursively exploring both truth value assignments, and backtracking upon encountering conflicts. This architectural change reduced space complexity from exponential to linear, making the approach practically viable for computational implementation and establishing the algorithmic foundation for virtually all modern SAT solvers.

Conflict-Driven Clause Learning (CDCL) represents the most significant algorithmic advancement in SAT solving since the original DPLL procedure, fundamentally transforming the efficiency and scalability of satisfiability solvers. The paradigm was pioneered independently by Marques-Silva and Sakallah through their GRASP solver~\cite{marques1999grasp} and by Bayardo and Schrag~\cite{bayardo1997using}, who demonstrated that incorporating constraint satisfaction problem look-back techniques could dramatically enhance SAT solver performance on real-world instances. The core innovation of CDCL lies in its sophisticated conflict analysis mechanism, which constructs implication graphs to systematically analyse the logical dependencies leading to conflicts during search~\cite{marques2009handbook}. When a conflict occurs, rather than simply backtracking chronologically as in traditional DPLL, CDCL algorithms analyse the implication graph to identify the Unique Implication Point (UIP) - the most recent decision variable that contributed to the conflict. This analysis enables the derivation of conflict clauses that precisely capture the logical reasons for the conflict, preventing the solver from repeating similar failed assignments. The non-chronological backtracking capability allows CDCL solvers to jump back multiple decision levels, directly to the point where the learned clause becomes unit, thus enabling immediate constraint propagation and dramatically improving search efficiency compared to basic DPLL implementations.

Modern SAT solvers incorporate sophisticated heuristics that build upon the CDCL framework to achieve remarkable performance on industrial instances. The Variable State Independent Decaying Sum (VSIDS) heuristic, introduced in the Chaff solver by Moskewicz et al.~\cite{moskewicz2001chaff}, represents a breakthrough in decision variable selection that prioritises variables recently involved in conflicts through an additive bumping and multiplicative decay scheme. VSIDS maintains activity scores for all variables, increasing scores for variables that appear in learned clauses whilst regularly reducing all scores by a fixed amount. This creates a system that prioritises variables involved in frequent recent conflicts over those that only occasionally cause problems~\cite{liang2015understanding}. Clause deletion strategies address the exponential growth of learned clauses by selectively removing less useful clauses based on metrics such as Literal Block Distance (LBD), which measures clause quality by counting the number of different decision levels represented in the clause~\cite{audemard2017learned}. Restart policies periodically erase the search tree whilst preserving learned clauses and variable activities, enabling solvers to escape unproductive search regions and explore different parts of the solution space~\cite{liang2018machine}. These complementary techniques work synergistically within the CDCL framework, with restart strategies enabling global exploration whilst clause learning provides local optimisation, and sophisticated variable ordering ensuring that search focuses on the most constrained and relevant variables, collectively enabling modern solvers to handle instances with millions of variables that would be completely intractable for earlier algorithmic approaches.

\subsection{Classical Graph Colouring Algorithms}
Classical graph colouring algorithms emerged from foundational research in combinatorial optimisation, establishing fundamental approaches for vertex assignment problems. The seminal work by Matula, Marble, and Isaacson~\cite{matula1972graph} provided a comprehensive analysis of early graph-colouring techniques, while Welsh and Powell's influential 1967 paper~\cite{welsh1967upper} introduced an upper bound for chromatic numbers and developed practical timetabling applications.

The basic greedy algorithm represents the simplest sequential approach, iteratively assigning vertices the smallest available colour not used by adjacent neighbours, achieving performance guarantees of at most $\Delta(G) + 1$ colours, where $\Delta(G)$ represents the maximum degree. The Welsh-Powell algorithm enhances this approach by ordering vertices in descending degree sequence, typically reducing the colour bound to $\max_i \min\{d(x_i) + 1, i\}$. More sophisticated approaches include the DSATUR algorithm, which dynamically prioritises vertices based on saturation degree---the number of differently coloured adjacent vertices~\cite{brelaz1979new}, and the Recursive Largest First (RLF) algorithm, which constructs colour classes by identifying maximal independent sets through specialised heuristic rules~\cite{leighton1979graph}. These foundational algorithms established the theoretical framework for further optimisation research in graph colouring.

SAT encodings for graph colouring translate the combinatorial problem into Boolean satisfiability instances, enabling the application of sophisticated SAT solver techniques to determine the ability to colour. Van Gelder's comprehensive analysis established three primary encoding strategies, each presenting distinct trade-offs between formula size and solver efficiency~\cite{vangelder2008another}.
Direct encoding represents the most intuitive approach, introducing Boolean variables $x_{v,c}$ for each vertex $v$ and colour $c$, where $x_{v,c} = \text{true}$ indicates vertex $v$ receives colour $c$. Constraints ensure each vertex receives exactly one colour through clauses $\bigvee_{c=1}^k x_{v,c}$ and prevent adjacent vertices from sharing colours via clauses $\neg x_{u,c} \vee \neg x_{v,c}$ for each edge $(u,v)$ and colour $c$. Although conceptually straightforward, this encoding generates $O(nk + mk)$ clauses for $n$ vertices, $m$ edges, and $k$ colours.
Order encoding provides alternative formulation using variables $y_{v,i}$ indicating vertex $v$ uses colour $i$ or lower, requiring fewer clauses but potentially complicating search dynamics~\cite{faber2024sat}. Advanced approaches incorporate symmetry-breaking mechanisms to reduce solution redundancy, with Dey and Bagchi demonstrating specialised techniques for constraint propagation optimisation~\cite{dey2013satisfiability}. Recent developments include partial-ordering models that achieve asymptotically smaller formula sizes compared to classical assignment-based approaches, while hybrid CP/SAT methods exploit structural decomposition for enhanced performance on large instances~\cite{hebrard2018clause}.

Symmetry-breaking techniques address the fundamental challenge that graph colouring problems possess inherent symmetries arising from colour permutations and vertex re-labellings, leading to exponentially many equivalent solutions that complicate search processes. Crawford et al. established the theoretical foundation for symmetry-breaking predicates (SBPs), demonstrating that while complete lexicographic leader constraints are NP-hard to compute, practical incomplete variants significantly enhance solver performance~\cite{crawford1996symmetry}. Lexicographic ordering constraints represent the most prevalent approach, imposing that colour assignments follow a predetermined ordering such as $c_1 \leq c_2 \leq \ldots \leq c_k$ for colours appearing in vertex sequence. Codish et al. developed specialised constraints for graph representation problems, demonstrating how structural properties can be exploited to reduce solution redundancy more effectively than generic symmetry-breaking approaches~\cite{codish2018constraints}. Dynamic symmetry breaking has emerged as a promising alternative, with Kirchweger et al. developing SAT Modulo Symmetries (SMS) approaches that integrate symmetry detection directly into CDCL solving procedures~\cite{kirchweger2021sat}. Recent advances include partial-ordering based constraints that achieve polynomial-size encodings while maintaining effectiveness, and hybrid static-dynamic methods that combine preprocessing optimisation with runtime symmetry detection~\cite{schaafsma2009dynamic}.

\subsection{Performance Optimisation Approaches}

Problem-specific heuristics exploit graph structural properties to guide SAT solver decision-making, significantly enhancing performance compared to generic variable ordering strategies. Hébrard and Katsirelos developed specialised decision strategies that emulate the well-established DSATUR heuristic within SAT frameworks, prioritising vertices based on saturation degree—the number of differently coloured adjacent vertices—thereby focussing search on the most constrained regions~\cite{hebrard2020constraint}. This approach leverages domain-specific knowledge to reduce search space exploration more effectively than traditional VSIDS-based ordering.

Recent advances include hybrid approaches combining multiple graph-theoretic criteria for variable selection, with largest-degree-first ordering integrated with saturation-based priorities to adapt to varying graph characteristics~\cite{catalyurek2012ordering}.Sun et al. developed a method that breaks massive graphs into small sections, uses SAT solvers to colour each section optimally, then combines the results—avoiding the problem of trying to encode enormous graphs that would overwhelm traditional SAT solvers~\cite{sun2023sat}. Zykov tree-based approaches offer an alternative paradigm by making relationship decisions between vertex pairs rather than direct colour assignments, though this method faces scalability challenges on sparse graphs due to the quadratic number of potential vertex relationships requiring consideration.

Preprocessing and simplification techniques systematically reduce graph colouring instances to smaller, equivalent problems before applying SAT solving, significantly improving computational efficiency through data reduction strategies. Lin et al. demonstrated that extracting large independent sets from massive graphs through dedicated tabu search algorithms substantially reduces problem size whilst preserving colouring properties, enabling more effective subsequent processing~\cite{lin2012coloring}. This approach removes vertices that can be trivially coloured, focusing computational effort on the challenging core structure.
Kernelisation theory provides a mathematical framework for systematically reducing graph colouring problems to smaller, equivalent forms. Jansen and Kratsch proved that any graph colouring problem can be reduced in polynomial time to a core problem of size at most $O(k^{q-1}\log k)$, where $k$ represents the vertex cover size and $q$ represents the number of colours needed. This size bound represents the best possible reduction achievable under standard computational complexity assumptions.~\cite{jansen2013data}. Advanced preprocessing includes dominated vertex elimination, where vertices with neighbourhoods entirely contained within another vertex's neighbourhood can be removed without affecting colourability. Modern SAT-based approaches integrate preprocessing directly into solving pipelines, with techniques such as clause subsumption, blocked clause elimination, and variable elimination reducing formula size before search begins~\cite{cao2021hash}, creating synergistic effects between structural graph simplification and Boolean formula optimisation.

Parallel and portfolio approaches exploit multiple processor cores to speed up SAT solving through coordinated execution strategies. Portfolio-based methods run several different SAT solver configurations simultaneously on the same problem, with the first to find a solution determining the result~\cite{hamadi2009manysat}. This approach works because different solvers perform better on different types of problems, and it is difficult to predict which solver will work best for any given instance~\cite{balyo2015hordesat}. ManySAT demonstrated the effectiveness of this strategy by winning the 2008 SAT-Race competition through careful parameter diversification and clause sharing between solver instances~\cite{hamadi2009manysat}. Recent developments include distributed portfolio systems for computing clusters and learning mechanisms that dynamically allocate computational resources based on solver performance patterns~\cite{guo2010diversification}.

\subsection{Research Gaps \& Opportunities}

Despite significant advances in both SAT solving and graph-colouring methodologies, several critical gaps remain that present opportunities for focused research contributions addressing the trade-off between average-case performance and worst-case robustness in moderate-scale problem domains. Current SAT-based graph colouring approaches predominantly pursue universal performance optimisation without systematic consideration of reliability guarantees on challenging instances~\cite{hebrard2020constraint}. Whilst sophisticated heuristics like VSIDS have proven effective for general SAT problems, they exhibit unpredictable behaviour on structurally challenging graph instances, creating deployment risks for reliability-critical applications.

The fundamental limitation in existing approaches lies in their optimisation philosophy: current research prioritises average-case performance improvements without addressing the brittleness that causes complete solver failures on challenging instances. Generic SAT solvers, when applied to graph colouring problems, can exhibit exponential degradation on dense connectivity patterns or high-degree vertices, leading to timeout failures that render them unsuitable for applications requiring guaranteed solution capability~\cite{marques2009handbook}. This reliability gap becomes particularly problematic in moderate-scale applications where solution failure represents unacceptable risk, such as academic scheduling systems, compiler optimisation pipelines, and resource allocation in critical infrastructure.

Existing research has concentrated primarily on either very large-scale industrial applications optimised for speed or theoretical algorithmic improvements focused on asymptotic complexity, leaving moderate-scale instances (50-100 vertices) under-explored despite their practical significance in reliability-critical domains. The gap between basic DPLL implementations and highly sophisticated CDCL engines presents an opportunity for targeted enhancements that incorporate graph-aware robustness optimisations without the complexity overhead that could introduce unpredictable behaviour characteristics~\cite{moskewicz2001chaff,marques2009handbook}. Current symmetry-breaking techniques, whilst mathematically sound, often impose significant computational overhead without providing corresponding robustness guarantees, suggesting opportunities for conservative optimisation strategies that prioritise reliability over aggressive reduction~\cite{crawford1996symmetry}.

The integration of preprocessing strategies with SAT solving remains largely perfor\-mance-focused rather than robustness-oriented, with kernelisation and independent set extraction typically performed to minimise problem size rather than enhance solver reliability on challenging instances~\cite{lin2012coloring,jansen2013data}. This presents an opportunity to develop preprocessing approaches that systematically invest computational overhead in structural analysis to prevent runtime failures, accepting predictable performance costs in exchange for worst-case reliability guarantees.

Most critically, existing literature lacks systematic investigation of performance-robustness trade-offs in SAT solving, with research typically pursuing either speed optimisation or theoretical completeness without characterising the systematic exchange between computational efficiency and operational reliability. This gap extends to evaluation methodologies, which predominantly focus on average-case performance metrics rather than reliability characteristics such as success rates on challenging instances, timeout avoidance, and graceful degradation behaviour under stress conditions.

This work addresses these gaps by developing a SAT solver architecture that explicitly prioritises worst-case robustness over average-case performance for moderate-scale graph colouring problems. The approach combines enhanced DPLL algorithms with comprehensive graph-aware preprocessing that accepts systematic performance overhead in exchange for reliability guarantees. Rather than pursuing universal optimisation, the research focuses on characterising and validating the systematic trade-off between computational efficiency and operational robustness, providing empirical evidence for deployment scenarios where solution reliability takes precedence over execution speed. This enables focused analysis of robustness optimisation strategies for structured combinatorial problems within reliability-critical application domains.