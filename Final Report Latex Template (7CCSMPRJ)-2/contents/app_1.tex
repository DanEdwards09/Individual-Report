\appendix

\section{Graph-Aware SAT Solving Implementation Code}
\label{appendix:graph-aware-implementation}

\subsection{Enhanced Solver Architecture for Trade-off Management}
\label{appendix:enhanced-solver-architecture}

\begin{lstlisting}[language=Python, caption=Enhanced Solver Architecture for Performance-Robustness Trade-offs]
class GraphAwareSATSolver(DPLLSolver):
    """
    Graph-aware SAT solver optimized for trading average-case performance 
    for worst-case robustness in graph coloring applications.
    """
    
    def __init__(self, robustness_mode: bool = True, verbose: bool = False):
        super().__init__(verbose=verbose)
        self.robustness_mode = robustness_mode
        
        # Trade-off management components
        self.overhead_tracker = OverheadTracker()
        self.robustness_monitor = RobustnessMonitor()
        
        # Graph-aware processing components
        self.graph_analyzer = None
        self.centrality_computer = None
        self.priority_manager = None
        
        # Performance-robustness statistics
        self.tradeoff_stats = {
            'preprocessing_overhead': 0.0,
            'analysis_time': 0.0,
            'enhanced_decisions': 0,
            'robustness_activations': 0,
            'fallback_events': 0,
            'success_rate_improvement': 0.0
        }
        
        # Configuration for controlled trade-off behavior
        self.overhead_budget = 1.5  # Maximum acceptable overhead factor
        self.robustness_threshold = 0.95  # Minimum success rate target
        
    def solve_with_tradeoff_analysis(self, vertices: List[int], 
                                   edges: List[Tuple[int, int]], 
                                   num_colors: int, 
                                   timeout: float = 300.0) -> TradeoffResult:
        """
        Main solving method with comprehensive trade-off analysis and validation.
        """
        start_time = time.time()
        
        # Initialize trade-off monitoring
        self.overhead_tracker.start_monitoring()
        self.robustness_monitor.initialize(vertices, edges, num_colors)
        
        # Phase 1: Graph analysis with overhead tracking
        if self.robustness_mode:
            analysis_result = self._perform_graph_analysis(vertices, edges)
            self.tradeoff_stats['analysis_time'] = analysis_result.time_cost
            self.tradeoff_stats['preprocessing_overhead'] = analysis_result.overhead_factor
        else:
            analysis_result = None
        
        # Phase 2: SAT encoding with robustness enhancements
        cnf_formula = self._encode_graph_coloring_with_enhancements(
            vertices, edges, num_colors, analysis_result
        )
        
        # Phase 3: Solving with trade-off monitoring
        solving_start = time.time()
        is_satisfiable, assignment = self._solve_with_robustness_tracking(
            cnf_formula, timeout - (solving_start - start_time)
        )
        solving_time = time.time() - solving_start
        
        # Phase 4: Trade-off analysis and result construction
        total_time = time.time() - start_time
        tradeoff_analysis = self._analyze_performance_robustness_tradeoff(
            total_time, solving_time, is_satisfiable
        )
        
        return TradeoffResult(
            satisfiable=is_satisfiable,
            assignment=self._convert_sat_to_coloring(assignment, vertices, num_colors),
            tradeoff_analysis=tradeoff_analysis,
            robustness_metrics=self.robustness_monitor.get_metrics(),
            overhead_breakdown=self.overhead_tracker.get_breakdown()
        )
\end{lstlisting}

\subsection{Threshold Analysis Implementation}
\label{appendix:threshold-analysis}

\begin{lstlisting}[language=Python, caption=Threshold Analysis for Trade-off Boundaries]
class ThresholdAnalyser:
    """
    Evaluates break-even points where enhanced solver overhead becomes 
    justified by reliability improvements across different operational contexts.
    """
    
    def __init__(self):
        self.threshold_results = []
        self.sensitivity_data = []
        
    def evaluate_deployment_thresholds(self, test_suite: List[Dict], 
                                     baseline_solver, enhanced_solver) -> Dict:
        """
        Systematic threshold identification for deployment guidance.
        """
        threshold_analysis = {
            'parameter_sweeps': {},
            'break_even_points': {},
            'deployment_recommendations': {},
            'sensitivity_analysis': {}
        }
        
        # Graph density threshold analysis
        density_thresholds = self._analyze_density_thresholds(
            test_suite, baseline_solver, enhanced_solver
        )
        threshold_analysis['parameter_sweeps']['density'] = density_thresholds
        
        # Problem size threshold analysis
        size_thresholds = self._analyze_size_thresholds(
            test_suite, baseline_solver, enhanced_solver
        )
        threshold_analysis['parameter_sweeps']['size'] = size_thresholds
        
        # Connectivity threshold analysis
        connectivity_thresholds = self._analyze_connectivity_thresholds(
            test_suite, baseline_solver, enhanced_solver
        )
        threshold_analysis['parameter_sweeps']['connectivity'] = connectivity_thresholds
        
        # Calculate break-even points
        threshold_analysis['break_even_points'] = self._calculate_break_even_points(
            threshold_analysis['parameter_sweeps']
        )
        
        # Generate deployment recommendations
        threshold_analysis['deployment_recommendations'] = self._generate_deployment_guidance(
            threshold_analysis['break_even_points']
        )
        
        # Perform sensitivity analysis
        threshold_analysis['sensitivity_analysis'] = self._perform_sensitivity_analysis(
            threshold_analysis['parameter_sweeps']
        )
        
        return threshold_analysis
    
    def _analyze_density_thresholds(self, test_suite: List[Dict], 
                                  baseline_solver, enhanced_solver) -> Dict:
        """
        Analyse how graph density affects trade-off attractiveness.
        """
        density_analysis = {
            'density_ranges': [],
            'overhead_ratios': [],

\subsection{Enhanced CDCL Solver Class Architecture}
\label{appendix:enhanced-cdcl-class}

\begin{lstlisting}[language=Python, caption=Enhanced CDCL Solver Class Architecture]
class EnhancedCDCLSolver(DPLLSolver):
    def __init__(self, enable_graph_awareness: bool = True, verbose: bool = False):
        super().__init__(verbose=verbose)
        self.enable_graph_awareness = enable_graph_awareness
        
        # Graph analysis components
        self.graph_analyzer = None
        self.variable_priority_order = None
        
        # Enhanced statistics and monitoring
        self.enhanced_stats = {
            'graph_analysis_time': 0.0,
            'preprocessing_reductions': 0,
            'enhanced_decisions': 0,
            'priority_cache_hits': 0,
            'fallback_activations': 0
        }
        
        # Performance tuning parameters
        self.centrality_weights = {'degree': 0.7, 'betweenness': 0.3}
        self.preprocessing_threshold = 50  # vertices
\end{lstlisting}

\subsection{Centrality-Based Variable Priority System}
\label{appendix:centrality-priority}

\begin{lstlisting}[language=Python, caption=Centrality-Based Variable Priority System]
def _initialize_graph_analysis(self, vertices: List[int], edges: List[Tuple[int, int]], 
                              num_colors: int) -> None:
    """Initialize graph analysis and compute variable priorities"""
    start_time = time.time()
    
    # Create graph analyzer with adaptive complexity management
    self.graph_analyzer = GraphStructureAnalyzer(vertices, edges)
    
    # Compute centrality measures based on problem scale
    if len(vertices) <= self.preprocessing_threshold:
        # Full analysis for smaller problems
        degree_centrality = self.graph_analyzer.compute_degree_centrality()
        betweenness_centrality = self.graph_analyzer.compute_betweenness_centrality()
    else:
        # Lightweight analysis for larger problems
        degree_centrality = self.graph_analyzer.compute_degree_centrality()
        betweenness_centrality = {v: 0.0 for v in vertices}  # Skip expensive computation
    
    # Compute composite vertex priorities
    vertex_priorities = {}
    for vertex in vertices:
        degree_score = degree_centrality.get(vertex, 0.0)
        betweenness_score = betweenness_centrality.get(vertex, 0.0)
        vertex_priorities[vertex] = (
            self.centrality_weights['degree'] * degree_score + 
            self.centrality_weights['betweenness'] * betweenness_score
        )
    
    # Map vertex priorities to SAT variable priorities
    self.variable_priority_order = []
    sorted_vertices = sorted(vertices, key=lambda v: vertex_priorities[v], reverse=True)
    
    for vertex in sorted_vertices:
        for color in range(num_colors):
            sat_variable = vertex * num_colors + color + 1
            self.variable_priority_order.append(sat_variable)
    
    analysis_time = time.time() - start_time
    self.enhanced_stats['graph_analysis_time'] = analysis_time
    
    if self.verbose:
        print(f"Graph analysis completed in {analysis_time:.3f}s")
        print(f"Computed priorities for {len(vertices)} vertices")
\end{lstlisting}

\subsection{Graph-Aware Search Algorithm Implementation}
\label{appendix:graph-aware-search}

\begin{lstlisting}[language=Python, caption=Graph-Aware Search Algorithm with Robustness Focus]
def _choose_variable_with_graph_awareness(self, cnf_formula: List[List[int]], 
                                        assignments: Dict[int, bool]) -> Optional[int]:
    """Enhanced variable selection using graph priorities with fallback mechanisms"""
    if not self.variable_priority_order:
        self.enhanced_stats['fallback_activations'] += 1
        return self._pick_unassigned_variable(cnf_formula, assignments)
    
    # Check priority cache for previously computed selections
    cache_key = tuple(sorted(assignments.keys()))
    if hasattr(self, 'priority_cache') and cache_key in self.priority_cache:
        self.enhanced_stats['priority_cache_hits'] += 1
        cached_variable = self.priority_cache[cache_key]
        if cached_variable not in assignments:
            return cached_variable
    
    # Find highest priority unassigned variable
    for variable in self.variable_priority_order:
        if variable not in assignments:
            # Cache the selection for future use
            if not hasattr(self, 'priority_cache'):
                self.priority_cache = {}
            self.priority_cache[cache_key] = variable
            return variable
    
    # Fallback to baseline selection if no priority variables remain
    self.enhanced_stats['fallback_activations'] += 1
    return self._pick_unassigned_variable(cnf_formula, assignments)

def _solve_with_robustness_tracking(self, cnf_formula: List[List[int]], 
                                  timeout: float) -> Tuple[bool, Dict[int, bool]]:
    """
    Core solving algorithm with robustness tracking and timeout protection.
    Implements graph-aware DPLL with comprehensive failure prevention.
    """
    start_time = time.time()
    assignments = {}
    decision_level = 0
    decision_stack = []
    
    while True:
        # Timeout protection for robustness
        if time.time() - start_time > timeout:
            self.enhanced_stats['timeout_occurred'] = True
            return False, {}
        
        # Unit propagation with enhanced tracking
        try:
            assignments = self._unit_propagate_with_tracking(cnf_formula, assignments)
        except Exception as e:
            self.enhanced_stats['propagation_errors'] = self.enhanced_stats.get('propagation_errors', 0) + 1
            return False, {}
        
        # Check formula status
        status = self._check_formula_status(cnf_formula, assignments)
        
        if status == "SATISFIED":
            self.enhanced_stats['successful_completion'] = True
            return True, assignments
        elif status == "UNSATISFIED":
            if decision_level == 0:
                return False, {}
            
            # Enhanced backtracking with conflict analysis
            try:
                assignments, decision_level, decision_stack = self._enhanced_backtrack(
                    assignments, decision_level, decision_stack, cnf_formula
                )
            except Exception as e:
                self.enhanced_stats['backtrack_errors'] = self.enhanced_stats.get('backtrack_errors', 0) + 1
                return False, {}
            continue
        
        # Graph-aware variable selection
        variable = self._choose_variable_with_graph_awareness(cnf_formula, assignments)
        if variable is None:
            return False, {}
        
        # Enhanced decision making with robustness considerations
        value = self._make_robust_decision(variable, cnf_formula, assignments)
        
        # Record decision and continue
        assignments[variable] = value
        decision_level += 1
        decision_stack.append((variable, value, decision_level))
        self.enhanced_stats['enhanced_decisions'] += 1

def _make_robust_decision(self, variable: int, 
                        cnf_formula: List[List[int]], 
                        assignments: Dict[int, bool]) -> bool:
    """
    Make variable assignment decisions optimised for robustness rather than speed.
    """
    # Count positive and negative occurrences in unresolved clauses
    positive_count = 0
    negative_count = 0
    
    for clause in cnf_formula:
        # Skip already satisfied clauses
        if any(assignments.get(abs(lit), None) == (lit > 0) for lit in clause 
               if abs(lit) in assignments):
            continue
        
        # Count occurrences in unresolved clauses
        if variable in clause:
            positive_count += 1
        elif -variable in clause:
            negative_count += 1
    
    # Robust heuristic: choose assignment that satisfies more clauses
    # In case of tie, prefer positive assignment for consistency
    if positive_count >= negative_count:
        return True
    else:
        return False
\end{lstlisting}

\subsection{Memory Management and Profiling Implementation}
\label{appendix:memory-management}

\begin{lstlisting}[language=Python, caption=Memory Management and Profiling System]
class MemoryProfiler:
    """
    Comprehensive memory profiling and management for SAT solver components.
    Tracks memory usage patterns to ensure predictable resource consumption.
    """
    
    def __init__(self, enable_profiling: bool = True):
        self.enable_profiling = enable_profiling
        self.memory_snapshots = []
        self.component_usage = defaultdict(int)
        self.peak_memory = 0
        self.baseline_memory = 0
    
    def track_component_memory(self, component_name: str, 
                             data_structure: Any) -> None:
        """Track memory usage of specific solver components"""
        if not self.enable_profiling:
            return
        
        memory_usage = self._estimate_structure_size(data_structure)
        self.component_usage[component_name] = memory_usage
        
        # Update peak memory tracking
        total_usage = sum(self.component_usage.values())
        self.peak_memory = max(self.peak_memory, total_usage)
    
    def _estimate_structure_size(self, obj: Any) -> int:
        """
        Estimate memory usage of data structures without external dependencies.
        Provides approximate but consistent memory tracking.
        """
        import sys
        
        if isinstance(obj, dict):
            size = sys.getsizeof(obj)
            for key, value in obj.items():
                size += sys.getsizeof(key) + sys.getsizeof(value)
            return size
        elif isinstance(obj, (list, tuple, set)):
            size = sys.getsizeof(obj)
            for item in obj:
                size += sys.getsizeof(item)
            return size
        else:
            return sys.getsizeof(obj)
    
    def take_memory_snapshot(self, operation_name: str) -> None:
        """Capture memory state at specific solver operations"""
        if not self.enable_profiling:
            return
        
        try:
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            
            snapshot = {
                'operation': operation_name,
                'rss_bytes': memory_info.rss,
                'vms_bytes': memory_info.vms,
                'component_breakdown': dict(self.component_usage),
                'timestamp': time.time()
            }
            
            self.memory_snapshots.append(snapshot)
            
        except ImportError:
            # Fallback tracking without psutil
            snapshot = {
                'operation': operation_name,
                'estimated_usage': sum(self.component_usage.values()),
                'component_breakdown': dict(self.component_usage),
                'timestamp': time.time()
            }
            
            self.memory_snapshots.append(snapshot)
    
    def generate_memory_report(self) -> Dict[str, Any]:
        """Generate comprehensive memory usage analysis"""
        if not self.memory_snapshots:
            return {'error': 'No memory snapshots available'}
        
        if 'rss_bytes' in self.memory_snapshots[0]:
            peak_memory = max(snapshot['rss_bytes'] for snapshot in self.memory_snapshots)
            memory_growth = (self.memory_snapshots[-1]['rss_bytes'] - 
                           self.memory_snapshots[0]['rss_bytes'])
            
            return {
                'peak_memory_mb': peak_memory / (1024 * 1024),
                'memory_growth_mb': memory_growth / (1024 * 1024),
                'component_breakdown': dict(self.component_usage),
                'snapshot_count': len(self.memory_snapshots),
                'profiling_overhead_estimate': len(self.memory_snapshots) * 0.1  # MB
            }
        else:
            # Fallback report format
            peak_estimated = max(snapshot['estimated_usage'] for snapshot in self.memory_snapshots)
            
            return {
                'peak_estimated_bytes': peak_estimated,
                'component_breakdown': dict(self.component_usage),
                'snapshot_count': len(self.memory_snapshots),
                'note': 'Estimated values (psutil unavailable)'
            }
    
    def reset_profiling(self):
        """Reset all profiling data for fresh measurement"""
        self.memory_snapshots.clear()
        self.component_usage.clear()
        self.peak_memory = 0
\end{lstlisting}

\subsection{Robust Conflict Analysis Implementation}
\label{appendix:robust-conflict-analysis}

\begin{lstlisting}[language=Python, caption=Robust Conflict Analysis for Consistent Performance]
def _enhanced_conflict_analysis(self, conflict_clause: List[int], 
                              decision_stack: List[Tuple[int, bool, int]]) -> List[int]:
    """
    Enhanced conflict analysis incorporating graph structural information
    for improved learned clause quality and robustness.
    """
    if not decision_stack or not self.variable_priority_order:
        return conflict_clause
    
    # Extract variables from recent decisions
    recent_variables = set()
    for variable, value, level in decision_stack[-5:]:  # Last 5 decisions
        recent_variables.add(abs(variable))
    
    # Prioritise high-priority variables in conflict clause
    priority_map = {var: idx for idx, var in enumerate(self.variable_priority_order)}
    
    conflict_literals = []
    for literal in conflict_clause:
        variable = abs(literal)
        if variable in recent_variables:
            # Include recent decisions in learned clause
            conflict_literals.append(-literal)
        elif variable in priority_map and priority_map[variable] < 20:
            # Include high-priority variables
            conflict_literals.append(-literal)
    
    # Ensure learned clause is non-empty and useful
    if not conflict_literals:
        return conflict_clause
    
    return conflict_literals

def _enhanced_backtrack(self, assignments: Dict[int, bool], 
                       decision_level: int, 
                       decision_stack: List[Tuple[int, bool, int]], 
                       cnf_formula: List[List[int]]) -> Tuple[Dict[int, bool], int, List]:
    """
    Enhanced backtracking with conflict analysis and robustness improvements.
    """
    if not decision_stack:
        return assignments, 0, []
    
    # Identify conflict and perform analysis
    conflict_clause = self._identify_conflict_clause(cnf_formula, assignments)
    learned_clause = self._enhanced_conflict_analysis(conflict_clause, decision_stack)
    
    # Backtrack to appropriate level
    backtrack_level = max(0, decision_level - 1)
    
    # Remove assignments from current level
    new_assignments = assignments.copy()
    new_decision_stack = []
    
    for variable, value, level in decision_stack:
        if level <= backtrack_level:
            new_decision_stack.append((variable, value, level))
        else:
            # Remove assignment from current level
            if variable in new_assignments:
                del new_assignments[variable]
    
    # Add learned clause to formula for future conflict avoidance
    if learned_clause and len(learned_clause) <= 10:  # Limit clause size for efficiency
        cnf_formula.append(learned_clause)
        self.enhanced_stats['learned_clauses'] = self.enhanced_stats.get('learned_clauses', 0) + 1
    
    return new_assignments, backtrack_level, new_decision_stack

def _identify_conflict_clause(self, cnf_formula: List[List[int]], 
                            assignments: Dict[int, bool]) -> List[int]:
    """
    Identify the conflict clause for enhanced conflict analysis.
    """
    for clause in cnf_formula:
        if all(assignments.get(abs(lit), None) is not None and 
               assignments[abs(lit)] != (lit > 0) for lit in clause):
            return clause
    
    # Return empty conflict if none found
    return []
\end{lstlisting}

\subsection{Integration Challenge Resolution Implementation}
\label{appendix:integration-challenges}

\begin{lstlisting}[language=Python, caption=Integration Challenge Resolution Implementation]
def _dpll_with_enhanced_ordering(self, cnf_formula: List[List[int]], 
                               assignments: Dict[int, bool]) -> Tuple[bool, Dict[int, bool]]:
    """DPLL with graph-aware variable ordering integration"""
    # Preserve original DPLL structure whilst enhancing variable selection
    propagated_formula, new_assignments = self._unit_propagation(cnf_formula, assignments.copy())
    assignments.update(new_assignments)
    
    # Maintain DPLL termination conditions
    if self._is_formula_satisfied(propagated_formula, assignments):
        return True, assignments
    
    if self._has_empty_clause(propagated_formula, assignments):
        return False, {}
    
    # Enhanced variable selection with fallback
    variable = self._choose_variable_with_graph_awareness(propagated_formula, assignments)
    if variable is None:
        return False, {}
    
    # Try positive assignment first
    positive_assignments = assignments.copy()
    positive_assignments[variable] = True
    
    satisfiable, solution = self._dpll_with_enhanced_ordering(
        propagated_formula, positive_assignments
    )
    
    if satisfiable:
        return True, solution
    
    # Try negative assignment
    negative_assignments = assignments.copy()
    negative_assignments[variable] = False
    
    return self._dpll_with_enhanced_ordering(propagated_formula, negative_assignments)

def _synchronize_with_baseline(self, baseline_solver: 'DPLLSolver') -> None:
    """
    Synchronise enhanced solver state with baseline for compatibility testing.
    Ensures consistent behaviour whilst maintaining robustness improvements.
    """
    # Synchronise common statistics
    if hasattr(baseline_solver, 'statistics'):
        shared_stats = {}
        for key in ['decisions', 'conflicts', 'unit_propagations']:
            if key in baseline_solver.statistics:
                shared_stats[key] = baseline_solver.statistics[key]
        
        # Merge with enhanced statistics
        self.enhanced_stats.update(shared_stats)
    
    # Synchronise timeout handling
    if hasattr(baseline_solver, 'timeout_occurred'):
        self.enhanced_stats['baseline_timeout'] = baseline_solver.timeout_occurred
    
    # Maintain interface compatibility
    if hasattr(baseline_solver, 'verbose'):
        self.verbose = baseline_solver.verbose

def _validate_interface_compatibility(self) -> Dict[str, bool]:
    """
    Validate that enhanced solver maintains interface compatibility
    with existing solver infrastructure.
    """
    compatibility_checks = {
        'dpll_solver_inheritance': isinstance(self, DPLLSolver),
        'solve_method_present': hasattr(self, 'solve'),
        'statistics_accessible': hasattr(self, 'get_statistics'),
        'verbose_mode_supported': hasattr(self, 'verbose'),
        'timeout_handling_present': 'timeout_occurred' in dir(self) or hasattr(self, 'enhanced_stats')
    }
    
    # Additional method signature checks
    import inspect
    
    if hasattr(self, 'solve'):
        solve_signature = inspect.signature(self.solve)
        compatibility_checks['solve_signature_valid'] = len(solve_signature.parameters) >= 1
    
    return compatibility_checks
\end{lstlisting>

\subsection{Comprehensive Statistics and Monitoring}
\label{appendix:comprehensive-statistics}

\begin{lstlisting}[language=Python, caption=Comprehensive Statistics and Monitoring Implementation]
def get_comprehensive_statistics(self) -> Dict[str, Any]:
    """Retrieve detailed solving statistics including graph-aware metrics"""
    base_stats = {}
    if hasattr(super(), 'get_statistics'):
        base_stats = super().get_statistics()
    
    combined_stats = {
        # Traditional SAT metrics
        'decisions': base_stats.get('decisions', 0),
        'conflicts': base_stats.get('conflicts', 0),
        'unit_propagations': base_stats.get('unit_propagations', 0),
        
        # Graph-aware enhancements
        'graph_analysis_time': self.enhanced_stats.get('graph_analysis_time', 0.0),
        'enhanced_decisions': self.enhanced_stats.get('enhanced_decisions', 0),
        'priority_cache_hits': self.enhanced_stats.get('priority_cache_hits', 0),
        'fallback_activations': self.enhanced_stats.get('fallback_activations', 0),
        'learned_clauses': self.enhanced_stats.get('learned_clauses', 0),
        
        # Robustness metrics
        'successful_completion': self.enhanced_stats.get('successful_completion', False),
        'timeout_occurred': self.enhanced_stats.get('timeout_occurred', False),
        'propagation_errors': self.enhanced_stats.get('propagation_errors', 0),
        'backtrack_errors': self.enhanced_stats.get('backtrack_errors', 0),
        
        # Performance ratios
        'enhancement_ratio': 0.0,
        'analysis_overhead': 0.0,
        'cache_hit_rate': 0.0
    }
    
    # Calculate derived metrics
    total_decisions = max(combined_stats['decisions'], 1)
    combined_stats['enhancement_ratio'] = (
        combined_stats['enhanced_decisions'] / total_decisions
    )
    
    total_time = self.enhanced_stats.get('total_time', 1.0)
    combined_stats['analysis_overhead'] = (
        combined_stats['graph_analysis_time'] / total_time
    )
    
    total_selections = combined_stats['enhanced_decisions'] + combined_stats['fallback_activations']
    if total_selections > 0:
        combined_stats['cache_hit_rate'] = (
            combined_stats['priority_cache_hits'] / total_selections
        )
    
    return combined_stats

def generate_performance_report(self) -> str:
    """
    Generate human-readable performance report for analysis and debugging.
    """
    stats = self.get_comprehensive_statistics()
    
    report = []
    report.append("=== Enhanced SAT Solver Performance Report ===")
    report.append("")
    
    # Basic solving statistics
    report.append("Basic Solving Statistics:")
    report.append(f"  Total Decisions: {stats['decisions']}")
    report.append(f"  Enhanced Decisions: {stats['enhanced_decisions']}")
    report.append(f"  Conflicts: {stats['conflicts']}")
    report.append(f"  Unit Propagations: {stats['unit_propagations']}")
    report.append(f"  Learned Clauses: {stats['learned_clauses']}")
    report.append("")
    
    # Graph-aware performance
    report.append("Graph-Aware Performance:")
    report.append(f"  Graph Analysis Time: {stats['graph_analysis_time']:.3f}s")
    report.append(f"  Priority Cache Hits: {stats['priority_cache_hits']}")
    report.append(f"  Fallback Activations: {stats['fallback_activations']}")
    report.append(f"  Cache Hit Rate: {stats['cache_hit_rate']:.2%}")
    report.append("")
    
    # Robustness metrics
    report.append("Robustness Metrics:")
    report.append(f"  Successful Completion: {stats['successful_completion']}")
    report.append(f"  Timeout Occurred: {stats['timeout_occurred']}")
    report.append(f"  Propagation Errors: {stats['propagation_errors']}")
    report.append(f"  Backtrack Errors: {stats['backtrack_errors']}")
    report.append("")
    
    # Performance ratios
    report.append("Performance Ratios:")
    report.append(f"  Enhancement Ratio: {stats['enhancement_ratio']:.2%}")
    report.append(f"  Analysis Overhead: {stats['analysis_overhead']:.2%}")
    report.append("")
    
    # Recommendations
    report.append("Recommendations:")
    if stats['fallback_activations'] > stats['enhanced_decisions'] * 0.2:
        report.append("  - Consider adjusting graph analysis threshold")
    if stats['cache_hit_rate'] < 0.3:
        report.append("  - Cache efficiency could be improved")
    if stats['analysis_overhead'] > 0.25:
        report.append("  - Graph analysis overhead is significant")
    if not stats['successful_completion'] and not stats['timeout_occurred']:
        report.append("  - Investigate solving failures")
    
    return "\n".join(report)
\end{lstlisting}

\section{Specialised Testing Components Code}
\label{appendix:specialised-testing-components}

\subsection{Stress Testing Framework Implementation}
\label{appendix:stress-testing}

\begin{lstlisting}[language=Python, caption=Stress Testing Framework for Worst-Case Validation]
class StressTester:
    """
    Generates increasingly challenging graph structures that systematically 
    push baseline solvers towards timeout failure whilst evaluating enhanced solver resilience.
    """
    
    def __init__(self, random_seed: int = 42):
        self.random_seed = random_seed
        self.stress_test_catalogue = []
        random.seed(random_seed)
        
    def generate_progressive_stress_tests(self, base_vertices: int = 50) -> List[Dict]:
        """
        Generate stress tests with progressive difficulty escalation.
        """
        stress_tests = []
        
        # Dense random graphs with increasing connectivity
        for density in [0.3, 0.5, 0.7, 0.8, 0.9]:
            stress_tests.append(self._create_dense_random_graph(base_vertices, density))
        
        # High-connectivity regular structures
        for degree in range(3, min(base_vertices // 2, 15)):
            stress_tests.append(self._create_regular_graph(base_vertices, degree))
        
        # Pathological instances designed to defeat baseline approaches
        stress_tests.extend(self._create_pathological_instances(base_vertices))
        
        return stress_tests
    
    def _create_dense_random_graph(self, n_vertices: int, density: float) -> Dict:
        """
        Create dense random graph designed to stress variable ordering heuristics.
        """
        vertices = list(range(n_vertices))
        edges = []
        
        # Generate edges based on density
        total_possible_edges = n_vertices * (n_vertices - 1) // 2
        target_edges = int(density * total_possible_edges)
        
        edge_set = set()
        attempts = 0
        while len(edge_set) < target_edges and attempts < target_edges * 10:
            u = random.randint(0, n_vertices - 1)
            v = random.randint(0, n_vertices - 1)
            if u != v:
                edge = tuple(sorted([u, v]))
                edge_set.add(edge)
            attempts += 1
        
        edges = list(edge_set)
        
        # Calculate chromatic number estimate (conservative upper bound)
        max_degree = max(len([e for e in edges if u in e]) for u in vertices)
        chromatic_estimate = max_degree + 1
        
        return {
            'id': f'dense_random_{n_vertices}_{density:.1f}',
            'vertices': vertices,
            'edges': edges,
            'colors': chromatic_estimate,
            'metadata': {
                'graph_type': 'dense_random',
                'density': density,
                'difficulty': 'stress',
                'expected_baseline_failure': density > 0.6
            }
        }
    
    def _create_regular_graph(self, n_vertices: int, degree: int) -> Dict:
        """
        Create regular graph with specified degree for connectivity stress testing.
        """
        if degree >= n_vertices:
            degree = n_vertices - 1
        
        vertices = list(range(n_vertices))
        edges = set()
        
        # Create regular structure
        for vertex in vertices:
            connections = 0
            offset = 1
            while connections < degree and offset < n_vertices:
                neighbour = (vertex + offset) % n_vertices
                if neighbour != vertex:
                    edge = tuple(sorted([vertex, neighbour]))
                    edges.add(edge)
                    connections += 1
                offset += 1
        
        edges = list(edges)
        
        return {
            'id': f'regular_{n_vertices}_{degree}',
            'vertices': vertices,
            'edges': edges,
            'colors': degree + 1,
            'metadata': {
                'graph_type': 'regular',
                'degree': degree,
                'difficulty': 'stress' if degree > 8 else 'moderate'
            }
        }
    
    def _create_pathological_instances(self, n_vertices: int) -> List[Dict]:
        """
        Create pathological instances specifically designed to expose 
        baseline solver limitations.
        """
        pathological_cases = []
        
        # Complete graph (worst case for graph colouring)
        if n_vertices <= 20:  # Only for small instances due to exponential growth
            vertices = list(range(n_vertices))
            edges = [(i, j) for i in range(n_vertices) for j in range(i + 1, n_vertices)]
            
            pathological_cases.append({
                'id': f'complete_{n_vertices}',
                'vertices': vertices,
                'edges': edges,
                'colors': n_vertices,
                'metadata': {
                    'graph_type': 'complete',
                    'difficulty': 'pathological',
                    'expected_baseline_failure': True
                }
            })
        
        # Highly connected bipartite-like structure
        partition_size = n_vertices // 2
        vertices = list(range(n_vertices))
        edges = []
        
        # Connect most vertices in first partition to most in second partition
        for i in range(partition_size):
            for j in range(partition_size, min(n_vertices, partition_size + 15)):
                edges.append((i, j))
        
        pathological_cases.append({
            'id': f'dense_bipartite_{n_vertices}',
            'vertices': vertices,
            'edges': edges,
            'colors': max(partition_size, 15),
            'metadata': {
                'graph_type': 'dense_bipartite',
                'difficulty': 'pathological'
            }
        })
        
        return pathological_cases
    
    def execute_stress_test_suite(self, baseline_solver, enhanced_solver, 
                                stress_tests: List[Dict], timeout: float = 15.0) -> Dict:
        """
        Execute stress test suite with failure mode documentation.
        """
        stress_results = {
            'total_tests': len(stress_tests),
            'baseline_failures': 0,
            'enhanced_failures': 0,
            'robustness_improvements': [],
            'failure_mode_analysis': []
        }
        
        for test_case in stress_tests:
            print(f"Executing stress test: {test_case['id']}")
            
            # Test baseline solver
            baseline_result = self._execute_with_failure_analysis(
                baseline_solver, test_case, timeout
            )
            
            # Test enhanced solver
            enhanced_result = self._execute_with_failure_analysis(
                enhanced_solver, test_case, timeout
            )
            
            # Analyse results
            if baseline_result['timed_out'] or not baseline_result['success']:
                stress_results['baseline_failures'] += 1
            
            if enhanced_result['timed_out'] or not enhanced_result['success']:
                stress_results['enhanced_failures'] += 1
            
            # Document robustness improvement
            if (baseline_result['timed_out'] and not enhanced_result['timed_out']):
                improvement = {
                    'test_id': test_case['id'],
                    'improvement_type': 'timeout_avoidance',
                    'baseline_failed': True,
                    'enhanced_succeeded': enhanced_result['success']
                }
                stress_results['robustness_improvements'].append(improvement)
            
            # Document failure modes
            failure_analysis = {
                'test_id': test_case['id'],
                'baseline_behaviour': baseline_result,
                'enhanced_behaviour': enhanced_result,
                'robustness_demonstrated': baseline_result['timed_out'] and not enhanced_result['timed_out']
            }
            stress_results['failure_mode_analysis'].append(failure_analysis)
        
        # Calculate summary statistics
        stress_results['baseline_success_rate'] = (
            (stress_results['total_tests'] - stress_results['baseline_failures']) / 
            stress_results['total_tests']
        )
        stress_results['enhanced_success_rate'] = (
            (stress_results['total_tests'] - stress_results['enhanced_failures']) / 
            stress_results['total_tests']
        )
        stress_results['robustness_improvement_count'] = len(stress_results['robustness_improvements'])
        
        return stress_results
    
    def _execute_with_failure_analysis(self, solver, test_case: Dict, timeout: float) -> Dict:
        """
        Execute solver with detailed failure mode analysis.
        """
        start_time = time.time()
        
        try:
            success, coloring, stats = solver.solve_graph_coloring(
                test_case['vertices'], test_case['edges'], test_case['colors'], timeout
            )
            
            execution_time = time.time() - start_time
            timed_out = execution_time >= timeout * 0.95  # 95% of timeout threshold
            
            return {
                'success': success,
                'execution_time': execution_time,
                'timed_out': timed_out,
                'solver_stats': stats,
                'failure_mode': 'timeout' if timed_out else ('unsatisfiable' if not success else None)
            }
            
        except Exception as e:
            return {
                'success': False,
                'execution_time': time.time() - start_time,
                'timed_out': False,
                'error': str(e),
                'failure_mode': 'exception'
            }
\end{lstlisting} = self._check_formula_status(cnf_formula, assignments)
        
        if status == "SATISFIED":
            self.enhanced_stats['successful_completion'] = True
            return True, assignments
        elif status == "UNSATISFIED":
            if decision_level == 0:
                return False, {}
            
            # Enhanced backtracking with conflict analysis
            try:
                assignments, decision_level, decision_stack = self._enhanced_backtrack(
                    assignments, decision_level, decision_stack, cnf_formula
                )
            except Exception as e:
                self.enhanced_stats['backtrack_errors'] = self.enhanced_stats.get('backtrack_errors', 0) + 1
                return False, {}
            continue
        
        # Graph-aware variable selection
        variable = self._choose_variable_with_graph_awareness(cnf_formula, assignments)
        if variable is None:
            return False, {}
        
        # Enhanced decision making with robustness considerations
        value = self._make_robust_decision(variable, cnf_formula, assignments)
        
        # Record decision and continue
        assignments[variable] = value
        decision_level += 1
        decision_stack.append((variable, value, decision_level))
        self.enhanced_stats['enhanced_decisions'] += 1
\end{lstlisting}

\subsection{Robustness-Oriented Graph Analysis Pipeline}
\label{appendix:robustness-oriented-analysis}

\begin{lstlisting}[language=Python, caption=Robustness-Oriented Graph Analysis Pipeline]
class RobustnessOrientedAnalyzer:
    """
    Graph analysis pipeline designed to maximize robustness insights
    while accepting computational overhead for comprehensive analysis.
    """
    
    def __init__(self, thoroughness_level: str = 'comprehensive'):
        self.thoroughness_level = thoroughness_level
        self.analysis_cache = {}
        self.computation_budget = self._set_computation_budget()
        
    def perform_comprehensive_analysis(self, vertices: List[int], 
                                     edges: List[Tuple[int, int]]) -> RobustnessAnalysis:
        """
        Comprehensive graph analysis optimized for robustness rather than speed.
        """
        analysis_start = time.time()
        
        # Phase 1: Structural characterization (accept overhead for completeness)
        structural_props = self._compute_comprehensive_structural_properties(vertices, edges)
        
        # Phase 2: Centrality analysis (thorough computation for robust priorities)
        centrality_measures = self._compute_multiple_centrality_measures(vertices, edges)
        
        # Phase 3: Connectivity analysis (identify potential failure points)
        connectivity_analysis = self._analyze_connectivity_robustness(vertices, edges)
        
        # Phase 4: Priority synthesis (combine multiple measures for robustness)
        robust_priorities = self._synthesize_robust_variable_priorities(
            structural_props, centrality_measures, connectivity_analysis
        )
        
        analysis_time = time.time() - analysis_start
        
        return RobustnessAnalysis(
            structural_properties=structural_props,
            centrality_measures=centrality_measures,
            connectivity_analysis=connectivity_analysis,
            variable_priorities=robust_priorities,
            analysis_overhead=analysis_time,
            robustness_confidence=self._assess_robustness_confidence(
                structural_props, connectivity_analysis
            )
        )
    
    def _compute_multiple_centrality_measures(self, vertices: List[int], 
                                           edges: List[Tuple[int, int]]) -> Dict[str, Dict[int, float]]:
        """
        Compute multiple centrality measures for robust variable prioritization.
        Accepts computational overhead for comprehensive structural understanding.
        """
        centrality_results = {}
        
        # Always compute degree centrality (fast baseline)
        centrality_results['degree'] = self._compute_degree_centrality(vertices, edges)
        
        # Compute betweenness centrality for moderate-scale problems
        if len(vertices) <= 100:  # Accept overhead for robustness
            centrality_results['betweenness'] = self._compute_betweenness_centrality(vertices, edges)
        else:
            centrality_results['betweenness'] = {v: 0.0 for v in vertices}
        
        # Compute closeness centrality for additional robustness
        if len(vertices) <= 80:  # Additional overhead for enhanced robustness
            centrality_results['closeness'] = self._compute_closeness_centrality(vertices, edges)
        else:
            centrality_results['closeness'] = {v: 0.0 for v in vertices}
        
        # Compute eigenvector centrality for comprehensive analysis
        if len(vertices) <= 60 and self.thoroughness_level == 'comprehensive':
            centrality_results['eigenvector'] = self._compute_eigenvector_centrality(vertices, edges)
        else:
            centrality_results['eigenvector'] = {v: 0.0 for v in vertices}
        
        return centrality_results
    
    def _synthesize_robust_variable_priorities(self, structural_props: Dict, 
                                             centrality_measures: Dict[str, Dict[int, float]], 
                                             connectivity_analysis: Dict) -> List[int]:
        """
        Synthesize variable priorities optimized for robustness rather than speed.
        """
        vertex_scores = {}
        
        for vertex in structural_props['vertices']:
            # Weighted combination emphasizing robustness indicators
            degree_score = centrality_measures['degree'].get(vertex, 0.0) * 0.4
            betweenness_score = centrality_measures['betweenness'].get(vertex, 0.0) * 0.3
            closeness_score = centrality_measures['closeness'].get(vertex, 0.0) * 0.2
            eigenvector_score = centrality_measures['eigenvector'].get(vertex, 0.0) * 0.1
            
            # Add robustness-specific bonuses
            if vertex in connectivity_analysis.get('critical_vertices', []):
                robustness_bonus = 0.2  # Prioritize structurally critical vertices
            else:
                robustness_bonus = 0.0
            
            vertex_scores[vertex] = (
                degree_score + betweenness_score + closeness_score + 
                eigenvector_score + robustness_bonus
            )
        
        # Return vertices sorted by robustness-oriented priority
        return sorted(structural_props['vertices'], 
                     key=lambda v: vertex_scores[v], reverse=True)
\end{lstlisting}

\subsection{Trade-off Monitoring and Analysis Framework}
\label{appendix:tradeoff-monitoring}

\begin{lstlisting}[language=Python, caption=Trade-off Monitoring and Analysis Framework]
class TradeoffAnalyzer:
    """
    Comprehensive framework for monitoring and analyzing performance-robustness trade-offs
    in graph-aware SAT solving applications.
    """
    
    def __init__(self):
        self.baseline_metrics = {}
        self.enhanced_metrics = {}
        self.tradeoff_history = []
        
    def analyze_comprehensive_tradeoff(self, baseline_result: SolverResult, 
                                     enhanced_result: SolverResult, 
                                     problem_characteristics: Dict) -> TradeoffAnalysis:
        """
        Comprehensive analysis of performance-robustness trade-offs with statistical validation.
        """
        # Performance impact analysis
        performance_analysis = self._analyze_performance_impact(baseline_result, enhanced_result)
        
        # Robustness improvement analysis
        robustness_analysis = self._analyze_robustness_improvements(baseline_result, enhanced_result)
        
        # Cost-benefit calculation
        cost_benefit = self._calculate_cost_benefit_ratio(performance_analysis, robustness_analysis)
        
        # Statistical significance testing
        statistical_validation = self._validate_tradeoff_significance(
            baseline_result, enhanced_result, problem_characteristics
        )
        
        # Deployment recommendation
        deployment_guidance = self._generate_deployment_recommendation(
            cost_benefit, statistical_validation, problem_characteristics
        )
        
        return TradeoffAnalysis(
            overhead_factor=performance_analysis['overhead_factor'],
            robustness_improvement=robustness_analysis['success_rate_improvement'],
            cost_benefit_ratio=cost_benefit,
            statistical_confidence=statistical_validation['confidence_level'],
            deployment_recommendation=deployment_guidance,
            detailed_metrics={
                'performance': performance_analysis,
                'robustness': robustness_analysis,
                'validation': statistical_validation
            }
        )
    
    def _analyze_performance_impact(self, baseline: SolverResult, 
                                  enhanced: SolverResult) -> Dict[str, float]:
        """
        Detailed analysis of performance overhead components and their contributions.
        """
        if baseline.execution_time == 0:
            overhead_factor = float('inf') if enhanced.execution_time > 0 else 1.0
        else:
            overhead_factor = enhanced.execution_time / baseline.execution_time
        
        preprocessing_overhead = enhanced.preprocessing_time / max(baseline.execution_time, 0.001)
        analysis_overhead = enhanced.analysis_time / max(baseline.execution_time, 0.001)
        search_overhead = (enhanced.search_time - baseline.search_time) / max(baseline.execution_time, 0.001)
        
        return {
            'overhead_factor': overhead_factor,
            'preprocessing_overhead_ratio': preprocessing_overhead,
            'analysis_overhead_ratio': analysis_overhead,
            'search_overhead_ratio': search_overhead,
            'total_overhead_seconds': enhanced.execution_time - baseline.execution_time
        }
    
    def _analyze_robustness_improvements(self, baseline: SolverResult, 
                                       enhanced: SolverResult) -> Dict[str, float]:
        """
        Quantitative analysis of robustness improvements and reliability enhancements.
        """
        baseline_success = 1.0 if baseline.satisfiable is not None else 0.0
        enhanced_success = 1.0 if enhanced.satisfiable is not None else 0.0
        
        success_rate_improvement = enhanced_success - baseline_success
        
        # Timeout avoidance analysis
        baseline_timeout = 1.0 if baseline.timed_out else 0.0
        enhanced_timeout = 1.0 if enhanced.timed_out else 0.0
        timeout_avoidance = baseline_timeout - enhanced_timeout
        
        # Decision efficiency analysis
        decision_efficiency = self._calculate_decision_efficiency(baseline, enhanced)
        
        return {
            'success_rate_improvement': success_rate_improvement,
            'timeout_avoidance': timeout_avoidance,
            'decision_efficiency': decision_efficiency,
            'reliability_score': (success_rate_improvement + timeout_avoidance) / 2.0
        }
    
    def _generate_deployment_recommendation(self, cost_benefit: float, 
                                          statistical_validation: Dict, 
                                          problem_characteristics: Dict) -> str:
        """
        Generate evidence-based deployment recommendations based on trade-off analysis.
        """
        confidence = statistical_validation['confidence_level']
        problem_difficulty = problem_characteristics.get('difficulty_rating', 'moderate')
        
        if cost_benefit > 0.5 and confidence > 0.8:
            if problem_difficulty == 'high':
                return 'strongly_recommend_enhanced'
            else:
                return 'recommend_enhanced'
        elif cost_benefit > 0.2 and confidence > 0.6:
            return 'conditionally_recommend_enhanced'
        elif cost_benefit > 0.0:
            return 'evaluate_case_by_case'
        else:
            return 'recommend_baseline'
\end{lstlisting}

\section{Core Graph Analysis Implementation Code}
\label{appendix:core-graph-analysis}

\subsection{GraphStructureAnalyzer Implementation}
\label{appendix:graph-structure-analyzer}

\begin{lstlisting}[language=Python, caption=Complete Graph Structure Analyzer Implementation]
class GraphStructureAnalyzer:
    """
    Comprehensive graph analysis for SAT solver optimisation.
    Provides all graph-theoretic analysis needed for implementing
    graph-aware heuristics without external dependencies.
    """
    
    def __init__(self, vertices: List[int], edges: List[Tuple[int, int]]):
        self.vertices = set(vertices)
        self.edges = set(edges)
        self.adjacency = defaultdict(set)
        self.degree_map = {}
        self._build_adjacency_structure()
    
    def _build_adjacency_structure(self):
        """Build efficient adjacency representation for graph operations"""
        for u, v in self.edges:
            self.adjacency[u].add(v)
            self.adjacency[v].add(u)
        
        # Compute degree map for O(1) degree access
        for vertex in self.vertices:
            self.degree_map[vertex] = len(self.adjacency[vertex])
    
    def compute_degree_centrality(self) -> Dict[int, float]:
        """
        Compute normalised degree centrality for all vertices.
        Critical for graph-aware variable ordering in SAT solving.
        """
        n = len(self.vertices)
        if n <= 1:
            return {v: 0.0 for v in self.vertices}
        
        centrality = {}
        for vertex in self.vertices:
            degree = self.degree_map[vertex]
            centrality[vertex] = degree / (n - 1)
        
        return centrality
    
    def compute_betweenness_centrality(self) -> Dict[int, float]:
        """
        Compute betweenness centrality using efficient shortest path algorithms.
        Identifies structurally important vertices for prioritised variable ordering.
        """
        centrality = {vertex: 0.0 for vertex in self.vertices}
        
        for source in self.vertices:
            # Single-source shortest paths using BFS
            stack = []
            paths = {vertex: [] for vertex in self.vertices}
            sigma = {vertex: 0.0 for vertex in self.vertices}
            distance = {vertex: -1 for vertex in self.vertices}
            delta = {vertex: 0.0 for vertex in self.vertices}
            
            sigma[source] = 1.0
            distance[source] = 0
            queue = deque([source])
            
            # BFS to find shortest paths
            while queue:
                vertex = queue.popleft()
                stack.append(vertex)
                
                for neighbour in self.adjacency[vertex]:
                    # First time we encounter this vertex
                    if distance[neighbour] < 0:
                        queue.append(neighbour)
                        distance[neighbour] = distance[vertex] + 1
                    
                    # Shortest path to neighbour via vertex
                    if distance[neighbour] == distance[vertex] + 1:
                        sigma[neighbour] += sigma[vertex]
                        paths[neighbour].append(vertex)
            
            # Accumulation phase
            while stack:
                vertex = stack.pop()
                for predecessor in paths[vertex]:
                    delta[predecessor] += (sigma[predecessor] / sigma[vertex]) * (1 + delta[vertex])
                
                if vertex != source:
                    centrality[vertex] += delta[vertex]
        
        # Normalisation for undirected graphs
        n = len(self.vertices)
        if n > 2:
            normalisation_factor = 2.0 / ((n - 1) * (n - 2))
            for vertex in centrality:
                centrality[vertex] *= normalisation_factor
        
        return centrality
    
    def compute_closeness_centrality(self) -> Dict[int, float]:
        """
        Compute closeness centrality for additional robustness metrics.
        """
        centrality = {}
        
        for vertex in self.vertices:
            # BFS for shortest path distances
            distances = {v: float('inf') for v in self.vertices}
            distances[vertex] = 0
            queue = deque([vertex])
            
            while queue:
                current = queue.popleft()
                for neighbour in self.adjacency[current]:
                    if distances[neighbour] == float('inf'):
                        distances[neighbour] = distances[current] + 1
                        queue.append(neighbour)
            
            # Calculate closeness centrality
            total_distance = sum(d for d in distances.values() if d != float('inf'))
            reachable_nodes = sum(1 for d in distances.values() if d != float('inf')) - 1
            
            if reachable_nodes > 0 and total_distance > 0:
                centrality[vertex] = reachable_nodes / total_distance
            else:
                centrality[vertex] = 0.0
        
        return centrality
    
    def analyze_structural_properties(self) -> Dict[str, float]:
        """
        Comprehensive structural analysis for adaptive parameter tuning.
        """
        n_vertices = len(self.vertices)
        n_edges = len(self.edges)
        
        # Basic graph metrics
        density = (2 * n_edges) / (n_vertices * (n_vertices - 1)) if n_vertices > 1 else 0
        average_degree = (2 * n_edges) / n_vertices if n_vertices > 0 else 0
        
        # Degree distribution analysis
        degrees = list(self.degree_map.values())
        max_degree = max(degrees) if degrees else 0
        min_degree = min(degrees) if degrees else 0
        degree_variance = sum((d - average_degree) ** 2 for d in degrees) / len(degrees) if degrees else 0
        
        return {
            'vertex_count': n_vertices,
            'edge_count': n_edges,
            'density': density,
            'average_degree': average_degree,
            'max_degree': max_degree,
            'min_degree': min_degree,
            'degree_variance': degree_variance,
            'connectivity_complexity': density * degree_variance
        }
\end{lstlisting}

% Add these to contents/app_1.tex

\subsection{Overhead Measurement Framework Implementation}
\label{appendix:overhead-measurement}

\begin{lstlisting}[language=Python, caption=Overhead Quantification and Consistency Analysis Framework]
class OverheadAnalyzer:
    """
    High-precision measurement protocols for isolating graph-analysis costs,
    priority-computation overhead, and search enhancement expenses.
    """
    
    def __init__(self, enable_detailed_profiling: bool = True):
        self.enable_detailed_profiling = enable_detailed_profiling
        self.component_timers = {}
        self.overhead_measurements = []
        self.consistency_data = []
    
    def measure_component_overhead(self, component_name: str, 
                                 baseline_time: float, 
                                 enhanced_time: float,
                                 problem_characteristics: Dict) -> Dict:
        """
        Isolate and quantify specific component overhead with statistical validation.
        """
        overhead_factor = enhanced_time / max(baseline_time, 0.001)
        absolute_overhead = enhanced_time - baseline_time
        
        overhead_measurement = {
            'component': component_name,
            'baseline_time': baseline_time,
            'enhanced_time': enhanced_time,
            'overhead_factor': overhead_factor,
            'absolute_overhead': absolute_overhead,
            'problem_size': problem_characteristics.get('vertex_count', 0),
            'problem_density': problem_characteristics.get('edge_density', 0.0),
            'timestamp': time.time()
        }
        
        self.overhead_measurements.append(overhead_measurement)
        return overhead_measurement
    
    def validate_overhead_consistency(self, measurements: List[Dict]) -> Dict:
        """
        Statistical validation of overhead consistency across problem categories.
        """
        if not measurements:
            return {'consistency_score': 0.0, 'statistical_significance': False}
        
        overhead_factors = [m['overhead_factor'] for m in measurements]
        mean_overhead = statistics.mean(overhead_factors)
        std_overhead = statistics.stdev(overhead_factors) if len(overhead_factors) > 1 else 0.0
        
        # Consistency validation: coefficient of variation should be low
        coefficient_of_variation = std_overhead / mean_overhead if mean_overhead > 0 else float('inf')
        consistency_score = max(0.0, 1.0 - coefficient_of_variation)
        
        # Statistical significance based on sample size and variance
        statistical_significance = len(measurements) >= 10 and coefficient_of_variation < 0.3
        
        return {
            'mean_overhead_factor': mean_overhead,
            'overhead_std_deviation': std_overhead,
            'coefficient_of_variation': coefficient_of_variation,
            'consistency_score': consistency_score,
            'statistical_significance': statistical_significance,
            'measurement_count': len(measurements),
            'overhead_bounds': (mean_overhead - 2*std_overhead, mean_overhead + 2*std_overhead)
        }
    
    def generate_overhead_stability_report(self) -> str:
        """Generate comprehensive overhead stability analysis report."""
        if not self.overhead_measurements:
            return "No overhead measurements available for analysis."
        
        report = ["=== Overhead Stability Analysis Report ===", ""]
        
        # Group measurements by component
        component_groups = {}
        for measurement in self.overhead_measurements:
            component = measurement['component']
            if component not in component_groups:
                component_groups[component] = []
            component_groups[component].append(measurement)
        
        # Analyse each component
        for component, measurements in component_groups.items():
            consistency_analysis = self.validate_overhead_consistency(measurements)
            
            report.append(f"Component: {component}")
            report.append(f"  Mean Overhead Factor: {consistency_analysis['mean_overhead_factor']:.3f}")
            report.append(f"  Consistency Score: {consistency_analysis['consistency_score']:.3f}")
            report.append(f"  Statistical Significance: {consistency_analysis['statistical_significance']}")
            report.append(f"  Measurement Count: {consistency_analysis['measurement_count']}")
            report.append("")
        
        return "\n".join(report)
\end{lstlisting}

\subsection{Stress Test Generation Framework}
\label{appendix:stress-generation}

\begin{lstlisting}[language=Python, caption=Adversarial Test Generation for Robustness Boundary Analysis]
class StressTester:
    """
    Generates increasingly challenging graph structures that systematically 
    push baseline solvers towards timeout failure whilst evaluating enhanced solver resilience.
    """
    
    def __init__(self, random_seed: int = 42):
        self.random_seed = random_seed
        self.stress_test_catalogue = []
        random.seed(random_seed)
    
    def generate_adversarial_instances(self, base_vertices: int = 50) -> List[Dict]:
        """
        Generate systematically challenging instances designed to defeat baseline approaches.
        """
        adversarial_tests = []
        
        # High-degree hub structures that stress variable ordering
        adversarial_tests.extend(self._create_hub_dominated_graphs(base_vertices))
        
        # Dense random graphs with pathological characteristics
        adversarial_tests.extend(self._create_pathological_dense_graphs(base_vertices))
        
        # Regular graphs with high chromatic number requirements
        adversarial_tests.extend(self._create_high_chromatic_regular_graphs(base_vertices))
        
        # Worst-case instances for specific heuristics
        adversarial_tests.extend(self._create_heuristic_defeating_instances(base_vertices))
        
        return adversarial_tests
    
    def _create_hub_dominated_graphs(self, n_vertices: int) -> List[Dict]:
        """Create graphs with high-degree hub vertices that stress variable ordering."""
        hub_graphs = []
        
        for hub_count in [2, 3, 5]:
            vertices = list(range(n_vertices))
            edges = []
            
            # Create hub vertices with high connectivity
            hub_vertices = vertices[:hub_count]
            regular_vertices = vertices[hub_count:]
            
            # Connect hubs to most other vertices
            for hub in hub_vertices:
                connection_probability = 0.8
                for other in vertices:
                    if other != hub and random.random() < connection_probability:
                        edges.append(tuple(sorted([hub, other])))
            
            # Add moderate connectivity between regular vertices
            for i, v1 in enumerate(regular_vertices):
                for v2 in regular_vertices[i+1:]:
                    if random.random() < 0.3:
                        edges.append(tuple(sorted([v1, v2])))
            
            # Remove duplicates
            edges = list(set(edges))
            
            hub_graphs.append({
                'id': f'hub_dominated_{n_vertices}_{hub_count}',
                'vertices': vertices,
                'edges': edges,
                'colors': hub_count + 3,  # Conservative estimate
                'metadata': {
                    'graph_type': 'hub_dominated',
                    'hub_count': hub_count,
                    'difficulty': 'adversarial',
                    'expected_baseline_failure': True,
                    'stress_characteristic': 'variable_ordering'
                }
            })
        
        return hub_graphs
    
    def _create_pathological_dense_graphs(self, n_vertices: int) -> List[Dict]:
        """Create dense graphs with characteristics that defeat common heuristics."""
        pathological_graphs = []
        
        # Near-clique with strategic gaps
        vertices = list(range(n_vertices))
        edges = []
        
        # Create near-complete graph
        for i in range(n_vertices):
            for j in range(i + 1, n_vertices):
                # Strategic gaps to create deceptive structure
                if not (i % 7 == 0 and j % 7 == 0 and abs(i - j) < 4):
                    edges.append((i, j))
        
        pathological_graphs.append({
            'id': f'pathological_dense_{n_vertices}',
            'vertices': vertices,
            'edges': edges,
            'colors': n_vertices - 2,  # Pessimistic estimate
            'metadata': {
                'graph_type': 'pathological_dense',
                'difficulty': 'adversarial',
                'expected_baseline_failure': True,
                'stress_characteristic': 'decision_making'
            }
        })
        
        return pathological_graphs
    
    def evaluate_stress_test_effectiveness(self, baseline_solver, enhanced_solver, 
                                         stress_tests: List[Dict], timeout: float = 15.0) -> Dict:
        """
        Evaluate the effectiveness of stress tests in demonstrating robustness improvements.
        """
        results = {
            'total_tests': len(stress_tests),
            'baseline_failures': 0,
            'enhanced_failures': 0,
            'robustness_demonstrations': [],
            'effectiveness_score': 0.0
        }
        
        for test_case in stress_tests:
            baseline_result = self._execute_stress_test(baseline_solver, test_case, timeout)
            enhanced_result = self._execute_stress_test(enhanced_solver, test_case, timeout)
            
            baseline_failed = baseline_result['timed_out'] or not baseline_result['success']
            enhanced_failed = enhanced_result['timed_out'] or not enhanced_result['success']
            
            if baseline_failed:
                results['baseline_failures'] += 1
            if enhanced_failed:
                results['enhanced_failures'] += 1
            
            # Document robustness demonstration
            if baseline_failed and not enhanced_failed:
                results['robustness_demonstrations'].append({
                    'test_id': test_case['id'],
                    'baseline_failure_mode': 'timeout' if baseline_result['timed_out'] else 'error',
                    'enhanced_success': enhanced_result['success'],
                    'improvement_magnitude': timeout - enhanced_result.get('execution_time', timeout)
                })
        
        # Calculate effectiveness score
        if results['baseline_failures'] > 0:
            results['effectiveness_score'] = len(results['robustness_demonstrations']) / results['baseline_failures']
        
        return results
\end{lstlisting}

\subsection{Threshold Analysis Framework}
\label{appendix:threshold-analysis}

\begin{lstlisting}[language=Python, caption=Cost-Benefit Threshold Analysis for Deployment Guidance]
class ThresholdAnalyser:
    """
    Evaluates break-even points where enhanced solver overhead becomes justified 
    by reliability improvements across different operational contexts.
    """
    
    def __init__(self, cost_model: Optional[Dict] = None):
        self.cost_model = cost_model or self._default_cost_model()
        self.threshold_data = []
        self.deployment_recommendations = {}
    
    def _default_cost_model(self) -> Dict:
        """Default cost model for threshold analysis."""
        return {
            'time_cost_factor': 1.0,  # Linear cost of execution time
            'failure_penalty': 10.0,  # Cost multiplier for solution failure
            'timeout_penalty': 5.0,   # Cost multiplier for timeout
            'reliability_value': 3.0  # Value multiplier for reliable solution
        }
    
    def compute_break_even_threshold(self, performance_overhead: float, 
                                   reliability_improvement: float,
                                   problem_characteristics: Dict) -> Dict:
        """
        Compute the break-even point for performance-reliability trade-off.
        """
        baseline_cost = self._calculate_baseline_cost(problem_characteristics)
        enhanced_cost = self._calculate_enhanced_cost(
            baseline_cost, performance_overhead, reliability_improvement
        )
        
        break_even_ratio = enhanced_cost / baseline_cost
        cost_benefit_score = self._calculate_cost_benefit_score(
            performance_overhead, reliability_improvement
        )
        
        threshold_analysis = {
            'performance_overhead': performance_overhead,
            'reliability_improvement': reliability_improvement,
            'baseline_cost': baseline_cost,
            'enhanced_cost': enhanced_cost,
            'break_even_ratio': break_even_ratio,
            'cost_benefit_score': cost_benefit_score,
            'recommendation': self._generate_threshold_recommendation(cost_benefit_score),
            'problem_characteristics': problem_characteristics
        }
        
        self.threshold_data.append(threshold_analysis)
        return threshold_analysis
    
    def _calculate_baseline_cost(self, problem_characteristics: Dict) -> float:
        """Calculate baseline operational cost."""
        base_time_cost = problem_characteristics.get('expected_time', 1.0) * self.cost_model['time_cost_factor']
        failure_risk = problem_characteristics.get('failure_probability', 0.1)
        failure_cost = failure_risk * self.cost_model['failure_penalty']
        
        return base_time_cost + failure_cost
    
    def _calculate_enhanced_cost(self, baseline_cost: float, 
                               overhead: float, reliability_improvement: float) -> float:
        """Calculate enhanced solver operational cost."""
        enhanced_time_cost = baseline_cost * overhead
        reliability_benefit = reliability_improvement * self.cost_model['reliability_value']
        
        return enhanced_time_cost - reliability_benefit
    
    def _calculate_cost_benefit_score(self, overhead: float, 
                                    reliability_improvement: float) -> float:
        """Calculate normalised cost-benefit score."""
        if overhead <= 1.0:
            return reliability_improvement  # Pure benefit if no overhead
        
        # Normalised score: benefit per unit overhead
        overhead_cost = overhead - 1.0
        if overhead_cost == 0:
            return float('inf') if reliability_improvement > 0 else 0.0
        
        return reliability_improvement / overhead_cost
    
    def _generate_threshold_recommendation(self, cost_benefit_score: float) -> str:
        """Generate deployment recommendation based on cost-benefit analysis."""
        if cost_benefit_score >= 2.0:
            return 'strongly_recommend_enhanced'
        elif cost_benefit_score >= 1.0:
            return 'recommend_enhanced'
        elif cost_benefit_score >= 0.5:
            return 'conditionally_recommend_enhanced'
        elif cost_benefit_score >= 0.0:
            return 'evaluate_case_by_case'
        else:
            return 'recommend_baseline'
    
    def generate_deployment_guidance_report(self) -> str:
        """Generate comprehensive deployment guidance based on threshold analysis."""
        if not self.threshold_data:
            return "No threshold analysis data available."
        
        report = ["=== Deployment Guidance Report ===", ""]
        
        # Aggregate analysis across all threshold data
        recommendations = [item['recommendation'] for item in self.threshold_data]
        recommendation_counts = {rec: recommendations.count(rec) for rec in set(recommendations)}
        
        report.append("Recommendation Distribution:")
        for recommendation, count in sorted(recommendation_counts.items()):
            percentage = (count / len(self.threshold_data)) * 100
            report.append(f"  {recommendation}: {count} cases ({percentage:.1f}%)")
        report.append("")
        
        # Cost-benefit analysis summary
        cost_benefit_scores = [item['cost_benefit_score'] for item in self.threshold_data]
        mean_score = statistics.mean(cost_benefit_scores)
        
        report.append(f"Average Cost-Benefit Score: {mean_score:.3f}")
        report.append(f"Score Range: {min(cost_benefit_scores):.3f} to {max(cost_benefit_scores):.3f}")
        report.append("")
        
        # Deployment recommendations
        report.append("Deployment Recommendations:")
        if mean_score >= 1.0:
            report.append("  - Enhanced solver deployment recommended for most scenarios")
            report.append("  - Strong cost-benefit justification demonstrated")
        elif mean_score >= 0.5:
            report.append("  - Enhanced solver suitable for reliability-critical applications")
            report.append("  - Case-by-case evaluation recommended")
        else:
            report.append("  - Enhanced solver benefits limited in current test scenarios")
            report.append("  - Focus on specific high-value use cases")
        
        return "\n".join(report)
\end{lstlisting}

\subsection{Independent Solution Validation Framework}
\label{appendix:independent-validation}

\begin{lstlisting}[language=Python, caption=External Constraint Satisfaction Verification System]
class SolutionValidator:
    """
    Independent solution verification that operates independently of both 
    baseline and enhanced solver internal mechanisms.
    """
    
    def __init__(self, enable_detailed_validation: bool = True):
        self.enable_detailed_validation = enable_detailed_validation
        self.validation_cache = {}
        self.validation_statistics = {
            'total_validations': 0,
            'successful_validations': 0,
            'failed_validations': 0,
            'cache_hits': 0
        }
    
    def validate_graph_coloring_solution(self, graph: Dict, 
                                       solution: Dict[int, int],
                                       expected_colors: int) -> Dict:
        """
        Comprehensive validation of graph coloring solution correctness.
        """
        validation_result = {
            'is_valid': True,
            'validation_errors': [],
            'solution_quality': {},
            'validation_time': 0.0
        }
        
        start_time = time.time()
        
        try:
            # Basic completeness check
            if not self._validate_solution_completeness(graph, solution):
                validation_result['is_valid'] = False
                validation_result['validation_errors'].append('incomplete_solution')
            
            # Constraint satisfaction validation
            if not self._validate_coloring_constraints(graph, solution):
                validation_result['is_valid'] = False
                validation_result['validation_errors'].append('constraint_violation')
            
            # Solution quality assessment
            validation_result['solution_quality'] = self._assess_solution_quality(
                graph, solution, expected_colors
            )
            
            # Update statistics
            self.validation_statistics['total_validations'] += 1
            if validation_result['is_valid']:
                self.validation_statistics['successful_validations'] += 1
            else:
                self.validation_statistics['failed_validations'] += 1
                
        except Exception as e:
            validation_result['is_valid'] = False
            validation_result['validation_errors'].append(f'validation_exception: {str(e)}')
        
        validation_result['validation_time'] = time.time() - start_time
        return validation_result
    
    def _validate_solution_completeness(self, graph: Dict, solution: Dict[int, int]) -> bool:
        """Verify that all vertices have been assigned colors."""
        graph_vertices = set(graph['vertices'])
        solution_vertices = set(solution.keys())
        return graph_vertices == solution_vertices
    
    def _validate_coloring_constraints(self, graph: Dict, solution: Dict[int, int]) -> bool:
        """Verify that no adjacent vertices share the same color."""
        for edge in graph['edges']:
            u, v = edge
            if solution.get(u) == solution.get(v) and solution.get(u) is not None:
                return False
        return True
    
    def _assess_solution_quality(self, graph: Dict, solution: Dict[int, int], 
                               expected_colors: int) -> Dict:
        """Assess the quality characteristics of the solution."""
        used_colors = set(solution.values())
        colors_used = len(used_colors)
        
        return {
            'colors_used': colors_used,
            'expected_colors': expected_colors,
            'color_efficiency': expected_colors / colors_used if colors_used > 0 else 0.0,
            'optimal_solution': colors_used <= expected_colors,
            'color_distribution': self._analyze_color_distribution(solution)
        }
    
    def _analyze_color_distribution(self, solution: Dict[int, int]) -> Dict:
        """Analyze the distribution of colors across vertices."""
        color_counts = {}
        for vertex, color in solution.items():
            color_counts[color] = color_counts.get(color, 0) + 1
        
        if not color_counts:
            return {'balance_score': 0.0, 'max_imbalance': 0}
        
        max_count = max(color_counts.values())
        min_count = min(color_counts.values())
        balance_score = min_count / max_count if max_count > 0 else 1.0
        
        return {
            'balance_score': balance_score,
            'max_imbalance': max_count - min_count,
            'color_counts': color_counts
        }
\end{lstlisting}

\subsection{Failure Mode Analysis Framework}
\label{appendix:failure-analysis}

\begin{lstlisting}[language=Python, caption=Systematic Failure Mode Documentation and Recovery Analysis]
class FailureAnalyser:
    """
    Captures decision sequences, conflict patterns, and backtracking behaviour 
    that distinguish robust performance from brittle failure.
    """
    
    def __init__(self, detailed_logging: bool = True):
        self.detailed_logging = detailed_logging
        self.failure_patterns = []
        self.recovery_demonstrations = []
        self.behavioural_comparisons = []
    
    def document_solver_behaviour(self, solver_result: Dict, 
                                solver_type: str, test_case: Dict) -> Dict:
        """
        Document detailed solver behaviour for failure analysis.
        """
        behaviour_profile = {
            'solver_type': solver_type,
            'test_case_id': test_case.get('id', 'unknown'),
            'execution_outcome': self._classify_execution_outcome(solver_result),
            'decision_characteristics': self._analyze_decision_patterns(solver_result),
            'failure_indicators': self._identify_failure_indicators(solver_result),
            'robustness_metrics': self._compute_robustness_metrics(solver_result)
        }
        
        if solver_result.get('timed_out', False):
            behaviour_profile['timeout_analysis'] = self._analyze_timeout_behaviour(solver_result)
        
        return behaviour_profile
    
    def compare_solver_behaviours(self, baseline_behaviour: Dict, 
                                enhanced_behaviour: Dict) -> Dict:
        """
        Compare baseline and enhanced solver behaviours to identify robustness improvements.
        """
        comparison = {
            'test_case_id': baseline_behaviour['test_case_id'],
            'outcome_comparison': self._compare_outcomes(baseline_behaviour, enhanced_behaviour),
            'decision_efficiency': self._compare_decision_efficiency(baseline_behaviour, enhanced_behaviour),
            'robustness_improvement': self._quantify_robustness_improvement(baseline_behaviour, enhanced_behaviour),
            'failure_recovery': self._analyze_failure_recovery(baseline_behaviour, enhanced_behaviour)
        }
        
        self.behavioural_comparisons.append(comparison)
        return comparison
    
    def _classify_execution_outcome(self, solver_result: Dict) -> str:
        """Classify the execution outcome for systematic analysis."""
        if solver_result.get('timed_out', False):
            return 'timeout'
        elif solver_result.get('satisfiable') is None:
            return 'unsolved'
        elif solver_result.get('satisfiable', False):
            return 'satisfiable'
        else:
            return 'unsatisfiable'
    
    def _analyze_decision_patterns(self, solver_result: Dict) -> Dict:
        """Analyze decision-making patterns for behavioural characterisation."""
        decisions = solver_result.get('decisions_made', 0)
        conflicts = solver_result.get('conflicts_encountered', 0)
        backtracks = solver_result.get('backtracks_performed', 0)
        
        if decisions == 0:
            return {'decision_efficiency': 0.0, 'conflict_rate': 0.0, 'backtrack_rate': 0.0}
        
        return {
            'decision_efficiency': 1.0 - (conflicts / decisions) if decisions > 0 else 0.0,
            'conflict_rate': conflicts / decisions if decisions > 0 else 0.0,
            'backtrack_rate': backtracks / decisions if decisions > 0 else 0.0,
            'total_decisions': decisions,
            'total_conflicts': conflicts,
            'total_backtracks': backtracks
        }
    
    def _identify_failure_indicators(self, solver_result: Dict) -> List[str]:
        """Identify indicators of solver failure or poor performance."""
        indicators = []
        
        if solver_result.get('timed_out', False):
            indicators.append('timeout_failure')
        
        decisions = solver_result.get('decisions_made', 0)
        conflicts = solver_result.get('conflicts_encountered', 0)
        
        if decisions > 0 and conflicts / decisions > 0.8:
            indicators.append('high_conflict_rate')
        
        if solver_result.get('execution_time', 0) > 10.0:
            indicators.append('excessive_runtime')
        
        if solver_result.get('memory_usage', 0) > 100 * 1024 * 1024:  # 100MB
            indicators.append('high_memory_usage')
        
        return indicators
    
    def _quantify_robustness_improvement(self, baseline: Dict, enhanced: Dict) -> Dict:
        """
        Quantify the robustness improvement demonstrated by enhanced solver.
        """
        baseline_failed = baseline['execution_outcome'] in ['timeout', 'unsolved']
        enhanced_failed = enhanced['execution_outcome'] in ['timeout', 'unsolved']
        
        if baseline_failed and not enhanced_failed:
            improvement_type = 'failure_recovery'
            improvement_magnitude = 1.0
        elif baseline_failed and enhanced_failed:
            improvement_type = 'no_improvement'
            improvement_magnitude = 0.0
        else:
            # Both succeeded - compare efficiency
            baseline_efficiency = baseline['decision_characteristics']['decision_efficiency']
            enhanced_efficiency = enhanced['decision_characteristics']['decision_efficiency']
            improvement_type = 'efficiency_improvement'
            improvement_magnitude = max(0.0, enhanced_efficiency - baseline_efficiency)
        
        return {
            'improvement_type': improvement_type,
            'improvement_magnitude': improvement_magnitude,
            'baseline_failed': baseline_failed,
            'enhanced_failed': enhanced_failed,
            'robustness_demonstrated': baseline_failed and not enhanced_failed
        }
    
    def generate_failure_analysis_report(self) -> str:
        """Generate comprehensive failure analysis report."""
        if not self.behavioural_comparisons:
            return "No behavioural comparison data available."
        
        report = ["=== Failure Mode Analysis Report ===", ""]
        
        # Count robustness demonstrations
        robustness_demonstrations = [
            comp for comp in self.behavioural_comparisons 
            if comp['robustness_improvement']['robustness_demonstrated']
        ]
        
        total_comparisons = len(self.behavioural_comparisons)
        demonstration_count = len(robustness_demonstrations)
        
        report.append(f"Total Behavioural Comparisons: {total_comparisons}")
        report.append(f"Robustness Demonstrations: {demonstration_count}")
        report.append(f"Robustness Improvement Rate: {demonstration_count/total_comparisons:.2%}")
        report.append("")
        
        # Analyze improvement types
        improvement_types = [comp['robustness_improvement']['improvement_type'] 
                           for comp in self.behavioural_comparisons]
        type_counts = {imp_type: improvement_types.count(imp_type) for imp_type in set(improvement_types)}
        
        report.append("Improvement Type Distribution:")
        for imp_type, count in sorted(type_counts.items()):
            percentage = (count / total_comparisons) * 100
            report.append(f"  {imp_type}: {count} cases ({percentage:.1f}%)")
        
        return "\n".join(report)
\end{lstlisting}

\subsection{Comprehensive Trade-off Evaluation Framework}
\label{appendix:comprehensive-evaluation}

\begin{lstlisting}[language=Python, caption=Multi-dimensional Trade-off Analysis and Characterisation]
class TradeoffEvaluator:
    """
    Generates detailed experimental data suitable for thesis-level validation 
    of graph-aware optimisation effectiveness.
    """
    
    def __init__(self, enable_statistical_validation: bool = True):
        self.enable_statistical_validation = enable_statistical_validation
        self.evaluation_data = []
        self.statistical_summaries = {}
        self.trade_off_models = {}
    
    def evaluate_comprehensive_trade_offs(self, baseline_results: List[Dict], 
                                        enhanced_results: List[Dict],
                                        problem_characteristics: List[Dict]) -> Dict:
        """
        Comprehensive evaluation of performance-robustness trade-offs across all test categories.
        """
        if len(baseline_results) != len(enhanced_results) or len(baseline_results) != len(problem_characteristics):
            raise ValueError("Result sets and problem characteristics must have matching lengths")
        
        comprehensive_analysis = {
            'total_evaluations': len(baseline_results),
            'performance_analysis': self._analyze_performance_characteristics(baseline_results, enhanced_results),
            'robustness_analysis': self._analyze_robustness_characteristics(baseline_results, enhanced_results),
            'trade_off_characterization': self._characterize_trade_off_relationships(
                baseline_results, enhanced_results, problem_characteristics
            ),
            'deployment_recommendations': self._generate_deployment_recommendations(
                baseline_results, enhanced_results, problem_characteristics
            )
        }
        
        # Statistical validation if enabled
        if self.enable_statistical_validation:
            comprehensive_analysis['statistical_validation'] = self._perform_statistical_validation(
                baseline_results, enhanced_results
            )
        
        self.evaluation_data.append(comprehensive_analysis)
        return comprehensive_analysis
    
    def _analyze_performance_characteristics(self, baseline_results: List[Dict], 
                                           enhanced_results: List[Dict]) -> Dict:
        """Analyze performance characteristics across all evaluations."""
        overhead_factors = []
        absolute_overheads = []
        
        for baseline, enhanced in zip(baseline_results, enhanced_results):
            if baseline.get('execution_time', 0) > 0:
                overhead_factor = enhanced.get('execution_time', 0) / baseline['execution_time']
                overhead_factors.append(overhead_factor)
                absolute_overheads.append(enhanced.get('execution_time', 0) - baseline['execution_time'])
        
        if not overhead_factors:
            return {'mean_overhead': 0.0, 'overhead_consistency': 0.0}
        
        mean_overhead = statistics.mean(overhead_factors)
        overhead_std = statistics.stdev(overhead_factors) if len(overhead_factors) > 1 else 0.0
        consistency_score = 1.0 - (overhead_std / mean_overhead) if mean_overhead > 0 else 0.0
        
        return {
            'mean_overhead_factor': mean_overhead,
            'overhead_standard_deviation': overhead_std,
            'overhead_consistency_score': consistency_score,
            'mean_absolute_overhead': statistics.mean(absolute_overheads) if absolute_overheads else 0.0,
            'overhead_range': (min(overhead_factors), max(overhead_factors)) if overhead_factors else (0.0, 0.0)
        }
    
    def _analyze_robustness_characteristics(self, baseline_results: List[Dict], 
                                          enhanced_results: List[Dict]) -> Dict:
        """Analyze robustness improvements across all evaluations."""
        success_improvements = 0
        timeout_preventions = 0
        total_evaluations = len(baseline_results)
        
        for baseline, enhanced in zip(baseline_results, enhanced_results):
            baseline_success = not (baseline.get('timed_out', False) or baseline.get('satisfiable') is None)
            enhanced_success = not (enhanced.get('timed_out', False) or enhanced.get('satisfiable') is None)
            
            if not baseline_success and enhanced_success:
                success_improvements += 1
            
            if baseline.get('timed_out', False) and not enhanced.get('timed_out', False):
                timeout_preventions += 1
        
        return {
            'success_improvement_rate': success_improvements / total_evaluations,
            'timeout_prevention_rate': timeout_preventions / total_evaluations,
            'robustness_score': (success_improvements + timeout_preventions) / total_evaluations,
            'total_robustness_demonstrations': success_improvements + timeout_preventions
        }
    
    def _characterize_trade_off_relationships(self, baseline_results: List[Dict], 
                                            enhanced_results: List[Dict],
                                            problem_characteristics: List[Dict]) -> Dict:
        """Characterize the relationship between performance cost and robustness benefit."""
        trade_off_points = []
        
        for baseline, enhanced, problem in zip(baseline_results, enhanced_results, problem_characteristics):
            performance_cost = self._calculate_performance_cost(baseline, enhanced)
            robustness_benefit = self._calculate_robustness_benefit(baseline, enhanced)
            
            trade_off_points.append({
                'performance_cost': performance_cost,
                'robustness_benefit': robustness_benefit,
                'cost_benefit_ratio': robustness_benefit / max(performance_cost, 0.001),
                'problem_size': problem.get('vertex_count', 0),
                'problem_complexity': problem.get('edge_density', 0.0)
            })
        
        # Analyze trade-off patterns
        cost_benefit_ratios = [point['cost_benefit_ratio'] for point in trade_off_points]
        
        return {
            'trade_off_points': trade_off_points,
            'mean_cost_benefit_ratio': statistics.mean(cost_benefit_ratios) if cost_benefit_ratios else 0.0,
            'cost_benefit_consistency': 1.0 - (statistics.stdev(cost_benefit_ratios) / statistics.mean(cost_benefit_ratios)) 
                                       if len(cost_benefit_ratios) > 1 and statistics.mean(cost_benefit_ratios) > 0 else 0.0,
            'positive_trade_offs': len([ratio for ratio in cost_benefit_ratios if ratio > 1.0]),
            'total_trade_offs': len(cost_benefit_ratios)
        }
    
    def _calculate_performance_cost(self, baseline: Dict, enhanced: Dict) -> float:
        """Calculate normalised performance cost."""
        baseline_time = baseline.get('execution_time', 0)
        enhanced_time = enhanced.get('execution_time', 0)
        
        if baseline_time <= 0:
            return 0.0
        
        return (enhanced_time - baseline_time) / baseline_time
    
    def _calculate_robustness_benefit(self, baseline: Dict, enhanced: Dict) -> float:
        """Calculate normalised robustness benefit."""
        baseline_success = not (baseline.get('timed_out', False) or baseline.get('satisfiable') is None)
        enhanced_success = not (enhanced.get('timed_out', False) or enhanced.get('satisfiable') is None)
        
        if baseline_success and enhanced_success:
            # Both succeeded - measure efficiency improvement
            baseline_decisions = baseline.get('decisions_made', 1)
            enhanced_decisions = enhanced.get('decisions_made', 1)
            return max(0.0, (baseline_decisions - enhanced_decisions) / baseline_decisions)
        elif not baseline_success and enhanced_success:
            # Failure recovery - maximum benefit
            return 1.0
        else:
            # No improvement or both failed
            return 0.0
    
    def _perform_statistical_validation(self, baseline_results: List[Dict], 
                                      enhanced_results: List[Dict]) -> Dict:
        """Perform statistical validation of trade-off effectiveness."""
        if len(baseline_results) < 10:
            return {'statistical_significance': False, 'reason': 'insufficient_sample_size'}
        
        # Paired t-test for execution time differences
        baseline_times = [r.get('execution_time', 0) for r in baseline_results if r.get('execution_time', 0) > 0]
        enhanced_times = [r.get('execution_time', 0) for r in enhanced_results if r.get('execution_time', 0) > 0]
        
        if len(baseline_times) != len(enhanced_times) or len(baseline_times) < 10:
            return {'statistical_significance': False, 'reason': 'insufficient_valid_measurements'}
        
        # Simple statistical validation without external dependencies
        time_differences = [e - b for b, e in zip(baseline_times, enhanced_times)]
        mean_difference = statistics.mean(time_differences)
        std_difference = statistics.stdev(time_differences) if len(time_differences) > 1 else 0.0
        
        # Simplified significance test
        t_statistic = mean_difference / (std_difference / (len(time_differences) ** 0.5)) if std_difference > 0 else 0.0
        
        return {
            'statistical_significance': abs(t_statistic) > 2.0,  # Simplified threshold
            'mean_time_difference': mean_difference,
            'standard_error': std_difference / (len(time_differences) ** 0.5),
            't_statistic': t_statistic,
            'sample_size': len(time_differences)
        }
    
    def generate_comprehensive_evaluation_report(self) -> str:
        """Generate comprehensive evaluation report for thesis documentation."""
        if not self.evaluation_data:
            return "No evaluation data available for comprehensive analysis."
        
        # Use the most recent evaluation
        latest_evaluation = self.evaluation_data[-1]
        
        report = ["=== Comprehensive Trade-off Evaluation Report ===", ""]
        
        # Performance analysis
        perf_analysis = latest_evaluation['performance_analysis']
        report.append("Performance Analysis:")
        report.append(f"  Mean Overhead Factor: {perf_analysis['mean_overhead_factor']:.3f}")
        report.append(f"  Overhead Consistency Score: {perf_analysis['overhead_consistency_score']:.3f}")
        report.append(f"  Mean Absolute Overhead: {perf_analysis['mean_absolute_overhead']:.3f}s")
        report.append("")
        
        # Robustness analysis
        rob_analysis = latest_evaluation['robustness_analysis']
        report.append("Robustness Analysis:")
        report.append(f"  Success Improvement Rate: {rob_analysis['success_improvement_rate']:.2%}")
        report.append(f"  Timeout Prevention Rate: {rob_analysis['timeout_prevention_rate']:.2%}")
        report.append(f"  Overall Robustness Score: {rob_analysis['robustness_score']:.3f}")
        report.append("")
        
        # Trade-off characterisation
        tradeoff_char = latest_evaluation['trade_off_characterization']
        report.append("Trade-off Characterisation:")
        report.append(f"  Mean Cost-Benefit Ratio: {tradeoff_char['mean_cost_benefit_ratio']:.3f}")
        report.append(f"  Positive Trade-offs: {tradeoff_char['positive_trade_offs']}/{tradeoff_char['total_trade_offs']}")
        report.append(f"  Trade-off Consistency: {tradeoff_char['cost_benefit_consistency']:.3f}")
        report.append("")
        
        # Statistical validation
        if 'statistical_validation' in latest_evaluation:
            stat_val = latest_evaluation['statistical_validation']
            report.append("Statistical Validation:")
            report.append(f"  Statistical Significance: {stat_val['statistical_significance']}")
            if stat_val['statistical_significance']:
                report.append(f"  Mean Time Difference: {stat_val['mean_time_difference']:.3f}s")
                report.append(f"  T-statistic: {stat_val['t_statistic']:.3f}")
            report.append("")
        
        return "\n".join(report)
\end{lstlisting}

\subsection{Scalability Testing Framework}
\label{appendix:scalability-testing}

\begin{lstlisting}[language=Python, caption=Scalability Analysis and Deployment Boundary Assessment]
class ScalabilityAnalyser:
    """
    Implements controlled scaling experiments that characterise how trade-off ratios 
    evolve with problem size and complexity.
    """
    
    def __init__(self, scaling_factors: List[float] = None):
        self.scaling_factors = scaling_factors or [0.5, 1.0, 1.5, 2.0, 2.5]
        self.scalability_data = []
        self.deployment_boundaries = {}
    
    def analyze_scalability_characteristics(self, base_problem: Dict, 
                                          baseline_solver, enhanced_solver,
                                          timeout: float = 30.0) -> Dict:
        """
        Analyze how trade-off characteristics evolve with increasing problem scale.
        """
        scaling_results = {
            'base_problem_id': base_problem.get('id', 'unknown'),
            'scaling_analysis': [],
            'scalability_trends': {},
            'deployment_boundaries': {}
        }
        
        for scale_factor in self.scaling_factors:
            scaled_problem = self._scale_problem(base_problem, scale_factor)
            
            # Test both solvers on scaled problem
            baseline_result = self._execute_scalability_test(baseline_solver, scaled_problem, timeout)
            enhanced_result = self._execute_scalability_test(enhanced_solver, scaled_problem, timeout)
            
            # Analyze scaling characteristics
            scale_analysis = self._analyze_scale_point(
                scale_factor, baseline_result, enhanced_result, scaled_problem
            )
            
            scaling_results['scaling_analysis'].append(scale_analysis)
        
        # Analyze trends across scale points
        scaling_results['scalability_trends'] = self._analyze_scalability_trends(
            scaling_results['scaling_analysis']
        )
        
        # Determine deployment boundaries
        scaling_results['deployment_boundaries'] = self._determine_deployment_boundaries(
            scaling_results['scaling_analysis']
        )
        
        self.scalability_data.append(scaling_results)
        return scaling_results
    
    def _scale_problem(self, base_problem: Dict, scale_factor: float) -> Dict:
        """Scale problem size while maintaining structural characteristics."""
        base_vertices = len(base_problem['vertices'])
        scaled_vertex_count = max(10, int(base_vertices * scale_factor))
        
        # Simple scaling: proportionally adjust vertex count and edge density
        original_edge_density = len(base_problem['edges']) / (base_vertices * (base_vertices - 1) / 2)
        
        scaled_vertices = list(range(scaled_vertex_count))
        scaled_edges = []
        
        # Generate edges based on original density
        import random
        random.seed(42)  # Reproducible scaling
        
        for i in range(scaled_vertex_count):
            for j in range(i + 1, scaled_vertex_count):
                if random.random() < original_edge_density:
                    scaled_edges.append((i, j))
        
        # Estimate chromatic number (conservative)
        max_degree = max(len([e for e in scaled_edges if v in e]) for v in scaled_vertices) if scaled_edges else 1
        scaled_colors = min(max_degree + 1, scaled_vertex_count)
        
        return {
            'id': f"{base_problem.get('id', 'scaled')}_{scale_factor:.1f}x",
            'vertices': scaled_vertices,
            'edges': scaled_edges,
            'colors': scaled_colors,
            'scale_factor': scale_factor,
            'original_problem': base_problem.get('id', 'unknown')
        }
    
    def _execute_scalability_test(self, solver, problem: Dict, timeout: float) -> Dict:
        """Execute solver with scalability-focused measurement."""
        start_time = time.time()
        
        try:
            # This would be replaced with actual solver execution
            # For framework demonstration, simulate execution characteristics
            vertex_count = len(problem['vertices'])
            edge_count = len(problem['edges'])
            
            # Simulate execution time based on problem complexity
            simulated_time = (vertex_count * 0.01) + (edge_count * 0.005)
            if simulated_time > timeout:
                return {
                    'execution_time': timeout,
                    'timed_out': True,
                    'satisfiable': None,
                    'decisions_made': int(vertex_count * 10),
                    'memory_usage': vertex_count * 1024
                }
            else:
                return {
                    'execution_time': simulated_time,
                    'timed_out': False,
                    'satisfiable': True,
                    'decisions_made': int(vertex_count * 5),
                    'memory_usage': vertex_count * 512
                }
                
        except Exception as e:
            return {
                'execution_time': time.time() - start_time,
                'timed_out': False,
                'satisfiable': None,
                'error': str(e)
            }
    
    def _analyze_scale_point(self, scale_factor: float, baseline_result: Dict, 
                           enhanced_result: Dict, problem: Dict) -> Dict:
        """Analyze trade-off characteristics at specific scale point."""
        baseline_time = baseline_result.get('execution_time', 0)
        enhanced_time = enhanced_result.get('execution_time', 0)
        
        overhead_factor = enhanced_time / max(baseline_time, 0.001)
        
        baseline_success = not baseline_result.get('timed_out', False)
        enhanced_success = not enhanced_result.get('timed_out', False)
        
        robustness_improvement = 0.0
        if not baseline_success and enhanced_success:
            robustness_improvement = 1.0
        elif baseline_success and enhanced_success:
            # Both succeeded - compare efficiency
            baseline_decisions = baseline_result.get('decisions_made', 1)
            enhanced_decisions = enhanced_result.get('decisions_made', 1)
            robustness_improvement = max(0.0, (baseline_decisions - enhanced_decisions) / baseline_decisions)
        
        return {
            'scale_factor': scale_factor,
            'problem_size': len(problem['vertices']),
            'overhead_factor': overhead_factor,
            'robustness_improvement': robustness_improvement,
            'baseline_success': baseline_success,
            'enhanced_success': enhanced_success,
            'cost_benefit_ratio': robustness_improvement / max(overhead_factor - 1.0, 0.001),
            'deployment_recommended': robustness_improvement > (overhead_factor - 1.0) * 0.5
        }
    
    def _analyze_scalability_trends(self, scaling_analysis: List[Dict]) -> Dict:
        """Analyze trends in scalability characteristics."""
        if not scaling_analysis:
            return {}
        
        scale_factors = [point['scale_factor'] for point in scaling_analysis]
        overhead_factors = [point['overhead_factor'] for point in scaling_analysis]
        robustness_improvements = [point['robustness_improvement'] for point in scaling_analysis]
        
        return {
            'overhead_trend': self._calculate_trend(scale_factors, overhead_factors),
            'robustness_trend': self._calculate_trend(scale_factors, robustness_improvements),
            'scalability_score': self._calculate_scalability_score(scaling_analysis),
            'optimal_scale_range': self._identify_optimal_scale_range(scaling_analysis)
        }
    
    def _calculate_trend(self, x_values: List[float], y_values: List[float]) -> Dict:
        """Calculate simple linear trend characteristics."""
        if len(x_values) < 2:
            return {'slope': 0.0, 'direction': 'stable'}
        
        # Simple linear regression slope calculation
        n = len(x_values)
        sum_x = sum(x_values)
        sum_y = sum(y_values)
        sum_xy = sum(x * y for x, y in zip(x_values, y_values))
        sum_x2 = sum(x * x for x in x_values)
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x) if (n * sum_x2 - sum_x * sum_x) != 0 else 0.0
        
        if slope > 0.1:
            direction = 'increasing'
        elif slope < -0.1:
            direction = 'decreasing'
        else:
            direction = 'stable'
        
        return {'slope': slope, 'direction': direction}
    
    def _determine_deployment_boundaries(self, scaling_analysis: List[Dict]) -> Dict:
        """Determine optimal deployment boundaries based on scaling analysis."""
        recommended_scales = [point for point in scaling_analysis if point['deployment_recommended']]
        
        if not recommended_scales:
            return {
                'has_viable_deployment_range': False,
                'recommended_scale_range': None,
                'deployment_guidance': 'Enhanced solver not recommended at tested scales'
            }
        
        min_viable_scale = min(point['scale_factor'] for point in recommended_scales)
        max_viable_scale = max(point['scale_factor'] for point in recommended_scales)
        
        return {
            'has_viable_deployment_range': True,
            'recommended_scale_range': (min_viable_scale, max_viable_scale),
            'optimal_scale_point': max(recommended_scales, key=lambda x: x['cost_benefit_ratio'])['scale_factor'],
            'deployment_guidance': f'Enhanced solver recommended for scale factors {min_viable_scale:.1f}x to {max_viable_scale:.1f}x'
        }
    
    def generate_scalability_report(self) -> str:
        """Generate comprehensive scalability analysis report."""
        if not self.scalability_data:
            return "No scalability data available for analysis."
        
        report = ["=== Scalability Analysis Report ===", ""]
        
        for analysis in self.scalability_data:
            report.append(f"Problem: {analysis['base_problem_id']}")
            
            trends = analysis['scalability_trends']
            report.append(f"  Overhead Trend: {trends.get('overhead_trend', {}).get('direction', 'unknown')}")
            report.append(f"  Robustness Trend: {trends.get('robustness_trend', {}).get('direction', 'unknown')}")
            report.append(f"  Scalability Score: {trends.get('scalability_score', 0.0):.3f}")
            
            boundaries = analysis['deployment_boundaries']
            report.append(f"  Deployment Viable: {boundaries.get('has_viable_deployment_range', False)}")
            if boundaries.get('has_viable_deployment_range', False):
                scale_range = boundaries.get('recommended_scale_range', (0, 0))
                report.append(f"  Recommended Scale Range: {scale_range[0]:.1f}x to {scale_range[1]:.1f}x")
            
            report.append("")
        
        return "\n".join(report)
\end{lstlisting}

% Centrality-Based Priority System Implementation (Referenced in Implementation Chapter)
\subsection{Centrality-Based Priority System Implementation}
\label{appendix:centrality-priority}

\begin{lstlisting}[language=Python, caption=Centrality-Based Variable Priority Calculation with Robustness Focus]
def compute_variable_priorities(self, graph: 'GraphStructure') -> List[int]:
    """
    Compute variable priorities based on graph centrality measures.
    Emphasises structural stability over computational efficiency.
    """
    if not graph.vertices:
        return []
    
    # Compute multiple centrality measures for robustness
    degree_centrality = graph.compute_degree_centrality()
    betweenness_centrality = graph.compute_betweenness_centrality()
    
    # Composite scoring with empirically validated weights
    composite_scores = {}
    for vertex in graph.vertices:
        degree_score = degree_centrality.get(vertex, 0.0)
        betweenness_score = betweenness_centrality.get(vertex, 0.0)
        
        # Robustness-optimised weighting (70% degree, 30% betweenness)
        composite_scores[vertex] = (0.7 * degree_score + 0.3 * betweenness_score)
    
    # Sort vertices by composite priority (highest first)
    sorted_vertices = sorted(composite_scores.keys(), 
                           key=lambda v: composite_scores[v], 
                           reverse=True)
    
    self.enhanced_stats['priority_computation_count'] += 1
    return sorted_vertices

def _validate_priority_consistency(self, priorities: List[int]) -> bool:
    """
    Validate priority ordering for structural consistency.
    Ensures reliable operation across diverse graph characteristics.
    """
    if len(priorities) != len(set(priorities)):
        return False
    
    # Verify all vertices represented
    expected_vertices = set(range(1, len(priorities) + 1))
    actual_vertices = set(abs(v) for v in priorities)
    
    return expected_vertices == actual_vertices
\end{lstlisting}

% Conflict Analysis Implementation (Referenced in Implementation Chapter)
\subsection{Streamlined Conflict Analysis Implementation}
\label{appendix:conflict-analysis}

\begin{lstlisting}[language=Python, caption=Graph-Aware Conflict Analysis with Robustness Guarantees]
def _analyse_conflict_with_graph_awareness(self, cnf_formula: List[List[int]], 
                                         assignments: Dict[int, bool]) -> List[int]:
    """
    Analyse conflicts incorporating graph structural information.
    Maintains effectiveness whilst avoiding complexity that could undermine robustness.
    """
    conflict_clause = self._identify_conflict_clause(cnf_formula, assignments)
    
    if not conflict_clause:
        return []
    
    # Incorporate high-priority variables into learned clauses
    enhanced_clause = conflict_clause.copy()
    recent_decisions = self._get_recent_high_priority_decisions(assignments)
    
    for variable in recent_decisions:
        if variable not in [abs(lit) for lit in conflict_clause]:
            # Add negation of recent high-priority assignment
            literal = -variable if assignments.get(variable, False) else variable
            enhanced_clause.append(literal)
    
    # Limit clause size for predictable performance
    if len(enhanced_clause) > self.max_learned_clause_size:
        enhanced_clause = self._prune_clause_by_priority(enhanced_clause)
    
    self.enhanced_stats['conflict_analysis_count'] += 1
    return enhanced_clause

def _get_recent_high_priority_decisions(self, assignments: Dict[int, bool]) -> List[int]:
    """
    Identify recent decisions involving structurally important vertices.
    """
    recent_variables = []
    for variable in self.variable_priority_order[:10]:  # Top 10 priority variables
        if variable in assignments:
            recent_variables.append(variable)
    
    return recent_variables[-5:]  # Most recent 5 high-priority decisions
\end{lstlisting}

% Performance Monitoring Implementation (Referenced in Implementation Chapter)
\subsection{Performance Monitoring and Statistics Collection}
\label{appendix:performance-monitoring}

\begin{lstlisting}[language=Python, caption=Comprehensive Performance Monitoring Infrastructure]
def _initialise_enhanced_statistics(self) -> Dict[str, Any]:
    """
    Initialise comprehensive statistics collection for trade-off analysis.
    Tracks both traditional SAT metrics and graph-specific indicators.
    """
    return {
        # Preprocessing overhead metrics
        'preprocessing_time': 0.0,
        'centrality_computation_time': 0.0,
        'priority_computation_count': 0,
        
        # Search behaviour metrics
        'enhanced_decisions': 0,
        'fallback_activations': 0,
        'priority_cache_hits': 0,
        'priority_cache_misses': 0,
        
        # Robustness indicators
        'conflict_analysis_count': 0,
        'backtrack_depth_sum': 0,
        'timeout_avoidance_count': 0,
        
        # Performance trade-off analysis
        'total_execution_time': 0.0,
        'overhead_percentage': 0.0,
        'success_on_challenging_instances': True
    }

def _update_preprocessing_statistics(self, start_time: float, end_time: float) -> None:
    """
    Update statistics with preprocessing performance data.
    """
    preprocessing_duration = end_time - start_time
    self.enhanced_stats['preprocessing_time'] += preprocessing_duration
    
    if hasattr(self, 'total_runtime') and self.total_runtime > 0:
        overhead_percentage = (preprocessing_duration / self.total_runtime) * 100
        self.enhanced_stats['overhead_percentage'] = overhead_percentage

def generate_performance_report(self) -> Dict[str, Any]:
    """
    Generate comprehensive performance analysis report.
    Characterises the performance-robustness trade-off.
    """
    return {
        'preprocessing_overhead': {
            'time_seconds': self.enhanced_stats['preprocessing_time'],
            'percentage_of_total': self.enhanced_stats['overhead_percentage']
        },
        'search_efficiency': {
            'enhanced_decisions': self.enhanced_stats['enhanced_decisions'],
            'cache_hit_rate': self._calculate_cache_hit_rate(),
            'fallback_frequency': self.enhanced_stats['fallback_activations']
        },
        'robustness_indicators': {
            'successful_completion': self.enhanced_stats['success_on_challenging_instances'],
            'conflict_analysis_effectiveness': self.enhanced_stats['conflict_analysis_count'],
            'timeout_avoidance': self.enhanced_stats['timeout_avoidance_count']
        }
    }
\end{lstlisting}

% Unit Propagation with Graph Awareness (Referenced but not detailed)
\subsection{Enhanced Unit Propagation Implementation}
\label{appendix:unit-propagation}

\begin{lstlisting}[language=Python, caption=Unit Propagation with Graph-Aware Optimisations]
def _unit_propagation_with_priorities(self, cnf_formula: List[List[int]], 
                                    assignments: Dict[int, bool]) -> Tuple[List[List[int]], Dict[int, bool]]:
    """
    Enhanced unit propagation that respects variable priorities.
    Maintains DPLL correctness whilst incorporating graph awareness.
    """
    new_assignments = assignments.copy()
    propagated_formula = []
    changes_made = True
    
    while changes_made:
        changes_made = False
        current_formula = []
        
        for clause in cnf_formula:
            satisfied = False
            unassigned_literals = []
            
            for literal in clause:
                variable = abs(literal)
                value = new_assignments.get(variable)
                
                if value is not None:
                    if (literal > 0 and value) or (literal < 0 and not value):
                        satisfied = True
                        break
                else:
                    unassigned_literals.append(literal)
            
            if satisfied:
                continue
            elif len(unassigned_literals) == 0:
                # Conflict detected
                return [], {}
            elif len(unassigned_literals) == 1:
                # Unit clause found - priority-aware propagation
                unit_literal = unassigned_literals[0]
                unit_variable = abs(unit_literal)
                unit_value = unit_literal > 0
                
                new_assignments[unit_variable] = unit_value
                changes_made = True
                self.enhanced_stats['unit_propagations'] += 1
            else:
                current_formula.append(clause)
        
        cnf_formula = current_formula
    
    return cnf_formula, new_assignments
\end{lstlisting}

% Timeout Protection and Graceful Degradation (Referenced in text)
\subsection{Timeout Protection and Graceful Degradation}
\label{appendix:timeout-protection}

\begin{lstlisting}[language=Python, caption=Robust Timeout Protection with Graceful Degradation]
def _solve_with_timeout_protection(self, cnf_formula: List[List[int]], 
                                 timeout: float) -> Tuple[bool, Dict[int, bool]]:
    """
    Solve with comprehensive timeout protection and graceful degradation.
    Ensures reliable operation in reliability-critical applications.
    """
    start_time = time.time()
    
    try:
        # Attempt enhanced solving with full graph awareness
        result = self._solve_with_enhanced_algorithm(cnf_formula, timeout, start_time)
        
        if result[0] or (time.time() - start_time) < timeout * 0.8:
            return result
        
        # Graceful degradation to baseline DPLL if timeout approaching
        self.enhanced_stats['graceful_degradation_count'] += 1
        remaining_time = timeout - (time.time() - start_time)
        
        if remaining_time > 0.1:  # Minimum time threshold
            return self._fallback_to_baseline_dpll(cnf_formula, remaining_time)
        
    except Exception as e:
        # Error recovery with baseline approach
        self.enhanced_stats['error_recovery_count'] += 1
        remaining_time = timeout - (time.time() - start_time)
        
        if remaining_time > 0.1:
            return self._fallback_to_baseline_dpll(cnf_formula, remaining_time)
    
    return False, {}

def _check_timeout_and_update_stats(self, start_time: float, timeout: float) -> bool:
    """
    Check timeout condition and update relevant statistics.
    """
    elapsed_time = time.time() - start_time
    
    if elapsed_time > timeout:
        self.enhanced_stats['timeout_occurrences'] += 1
        return True
    
    # Update progress statistics
    progress_percentage = (elapsed_time / timeout) * 100
    if progress_percentage > 50 and not hasattr(self, '_halfway_logged'):
        self.enhanced_stats['halfway_progress_count'] += 1
        self._halfway_logged = True
    
    return False
\end{lstlisting}